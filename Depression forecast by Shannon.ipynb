{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5db858c-7a09-474b-a267-32bf55d7a615",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This notebook is for predicting postpartum depression from Shannon entropy.\n",
    "The results from this notebook are presented in Table 7 in the paper\"\"\"\n",
    "###import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_selection import (f_classif,SelectKBest)\n",
    "import scipy.stats as stats\n",
    "from scipy.cluster import hierarchy\n",
    "import warnings\n",
    "###import libraries for log-transformation and imputation\n",
    "import sys\n",
    "sys.path.insert(0, '/libraries')\n",
    "from libraries import Transformation_library\n",
    "###fix random seed for reproducibility \n",
    "import os\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f65ddce-906c-43d4-ba28-6f2c7c40f89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###user-defined parameters\n",
    "is_balanced = 0 #do we want to use weights during classification? (as discussed in section 4.1)\n",
    "is_median = 1 #do we want to use median or mean when select optimal n (number of features, as discussed in section 4.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45105d29-95eb-4e0a-8fe5-fd18145c4aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "###standard parameters\n",
    "level = 0.5 #balanced accuracy when predictions are made by random guess\n",
    "test = 0.2 #fraction of data used for test set\n",
    "val = 0.2 #fraction for validation (from the rest part)\n",
    "ITER = 5 #number of iteration for random forest (section 3.5)\n",
    "n_features = 2 #the maximum number of features when selecting by ANOVA (as discussed in section 3.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8e3e42-ecd9-49b9-bd04-d9f94d874cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sets = int(1/test) #number of testing sets (5)\n",
    "val_sets = int(1/val) #number of validation sets (5)\n",
    "binary = 11 #threshold to be classified as helthy/depressed \n",
    "if is_balanced == 1:\n",
    "    class_weight = \"balanced\"\n",
    "else:\n",
    "    class_weight = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9cdb98-7ab5-4d7e-810c-32a0abe9249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read metadata file with id of participants and time points (depending on dataset number)\n",
    "Metadata=pd.read_csv(\"BASIC_metadata_full.csv\",sep=',',low_memory=False) #read file with depression levels and ids of participants\n",
    "Metadata.loc[Metadata.TimePoint==\"Trimester2\",\"TimePoint\"] = 0 #timepoits are 0,1,2\n",
    "Metadata.loc[Metadata.TimePoint==\"Trimester3\",\"TimePoint\"] = 1\n",
    "Metadata.loc[Metadata.TimePoint==\"PostpartumWeek6\",\"TimePoint\"] = 2\n",
    "i = Metadata[Metadata.ReadsNumber < 500000].index #remove insufficient reads\n",
    "Metadata.loc[i, 'ReadsNumber'] = np.nan\n",
    "EPDS = 2*np.ones_like(Metadata.EPDS) #2 is for missingness\n",
    "EPDS[Metadata.EPDS>binary] = 1 #binary labels for depression\n",
    "EPDS[Metadata.EPDS<=binary] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312d1e4a-f553-448f-96ba-11ab1833f71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###data\n",
    "profile =pd.read_csv(\"Species_Profile_full.csv\",sep=',',low_memory=False) #read file with compositional data\n",
    "species=profile.to_numpy()[:,1:]/100 #normilized to 1\n",
    "ID = profile.Sample_id.to_numpy() #id of a sample in profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c218686-9fcb-4c67-8ed0-d526c70e9895",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full, labels = [[] for _ in range(2)] #X is 2 values of diversity index, labels are EPDS at tp 2\n",
    "Individual_ID = Metadata.Individual_ID #id of a person\n",
    "for i in np.unique(Individual_ID):\n",
    "    TP = Metadata.TimePoint[Individual_ID == i].to_numpy()\n",
    "    if sum((TP - np.array([0,1,2]))**2)!=0:\n",
    "        warnings.warn(\"TP are missing or in incorrect order\") #does not happen in the dataset we have\n",
    "    Outcomes = EPDS[Individual_ID == i]\n",
    "    if Outcomes[2]!=2:\n",
    "        Reads = Metadata.ReadsNumber[Individual_ID == i].to_numpy()\n",
    "        if 1-np.isnan(Reads[0]) and 1-np.isnan(Reads[1]):\n",
    "            labels = np.append(labels, Outcomes[2])\n",
    "            Sample_ID = Metadata.Sample_ID[Individual_ID == i]  # id of a sample in metadata\n",
    "            Dataset_line = np.zeros((1,2))\n",
    "            for j in range(2):\n",
    "                tp_id = Sample_ID[TP == j].to_numpy()\n",
    "                prob_full = species[np.where(ID == tp_id)[0], :]\n",
    "                prob = prob_full[prob_full>0]\n",
    "                Dataset_line[0, j] = -sum(list(map(lambda x: x*np.log(x), prob)))\n",
    "            X_full = Dataset_line if np.size(X_full) == 0 else np.concatenate([X_full, Dataset_line])\n",
    "X_f, X_t = X_full[labels == 0], X_full[labels == 1] #f = healthy, t = depressed\n",
    "size_f, size_t = np.shape(X_f)[0], np.shape(X_t)[0]\n",
    "print(\"sizes of complete sets:\",size_f,size_t)\n",
    "test_f, test_t = int(size_f*test), int(size_t*test) # sizes of test sets\n",
    "AUC_TEST, N_OPT, SENSITIVITY, SPECIFICITY = [[] for _ in range(4)]\n",
    "aucs_val = np.zeros((val_sets, ITER, n_features))\n",
    "for TS in range(test_sets): #iterating over test sets\n",
    "    print(\"test group\", TS)\n",
    "    test_f_range, test_t_range = range(TS * test_f, (TS + 1) * test_f), range(TS * test_t, (TS + 1) * test_t)\n",
    "    rest_f_range, rest_t_range = np.setdiff1d(range(size_f), test_f_range), np.setdiff1d(range(size_t), test_t_range)\n",
    "    X_f_test, X_t_test = X_f[test_f_range], X_t[test_t_range]\n",
    "    X_test = np.concatenate([X_f_test,X_t_test])\n",
    "    label_test = np.zeros(test_f + test_t)\n",
    "    label_test[test_f:] = 1\n",
    "    rest_f, rest_t = np.size(rest_f_range), np.size(rest_t_range)\n",
    "    val_f, val_t = int(rest_f * val), int(rest_t * val) #sizes of validation sets\n",
    "    train_f, train_t = rest_f - val_f, rest_t - val_t #the rest of rest is for training set\n",
    "    print(\"sizes of train sets\", train_f, train_t)\n",
    "    for VS in range(val_sets): #iterating over vaidation sets\n",
    "        print(\"val group\", VS)\n",
    "        val_f_range, val_t_range = rest_f_range[range(VS* val_f, (VS + 1) * val_f)], rest_t_range[range(VS* val_t, (VS + 1) * val_t)]\n",
    "        train_f_range, train_t_range = np.setdiff1d(rest_f_range, val_f_range), np.setdiff1d(rest_t_range, val_t_range)\n",
    "        X_f_val, X_t_val, X_f_train, X_t_train = X_f[val_f_range], X_t[val_t_range], X_f[train_f_range], X_t[train_t_range]\n",
    "        X_val, X_train = np.concatenate([X_f_val, X_t_val]), np.concatenate([X_f_train, X_t_train])\n",
    "        label_val, label_train = np.zeros(val_f + val_t), np.zeros(train_f + train_t)\n",
    "        label_val[val_f:], label_train[train_f:] = 1, 1\n",
    "        stats_train, stats_val = X_train, X_val\n",
    "        n_features = np.min([n_features, np.shape(stats_train)[1]])\n",
    "        for j in range(n_features):  #choose the best j + 1 features using ANOVA (section 3.2)\n",
    "            sel = SelectKBest(f_classif, k=j + 1).fit(pd.DataFrame(stats_train), label_train)\n",
    "            Index = sel.get_support()\n",
    "            for seed in range(ITER): #classify by random forest with different random seeds (section 3.4)\n",
    "                rf = RandomForestClassifier(random_state = seed, class_weight=class_weight).fit(stats_train[:,Index], label_train)\n",
    "                y_pred = rf.predict(stats_val[:,Index])\n",
    "                aucs_val[VS, seed, j] = roc_auc_score(label_val, y_pred)\n",
    "    if is_median == 1: #optimal number of features by median of mean\n",
    "        print(\"medians\", np.median(aucs_val, axis=(0, 1))) #average by validation sets and random seeds\n",
    "        n_opt = np.argmax(np.median(aucs_val, axis=(0, 1))) + 1\n",
    "    else:\n",
    "        print(\"means\", np.mean(aucs_val, axis = (0,1)))\n",
    "        n_opt=np.argmax(np.mean(aucs_val, axis = (0,1))) + 1\n",
    "    N_OPT =np.append(N_OPT, n_opt)\n",
    "    X_rest, label_rest = np.concatenate([X_train, X_val]), np.concatenate([label_train,label_val])#concatenate everything except testing set\n",
    "    stats_rest, stats_test = X_rest, X_test\n",
    "    sel = SelectKBest(f_classif, k=n_opt).fit(pd.DataFrame(stats_rest), label_rest) #select optimal number of features\n",
    "    Index = sel.get_support()\n",
    "    print(\"test features:\",sel.get_feature_names_out()) #from features namas we can reconstruct what bacteria we use\n",
    "    for seed in range(ITER): #iterations for random forest on test sets\n",
    "        rf = RandomForestClassifier(random_state = seed, class_weight=class_weight).fit(stats_rest[:, Index], label_rest)\n",
    "        print(\"importances\",rf.feature_importances_) #display importances of features\n",
    "        y_pred = rf.predict(stats_test[:, Index])\n",
    "        print(\"auc\", roc_auc_score(label_test, y_pred)) #display balanced accuracy\n",
    "        AUC_TEST = np.append(AUC_TEST, roc_auc_score(label_test, y_pred)) # balanced accuracy on testing set\n",
    "        SENSITIVITY = np.append(SENSITIVITY, sum(y_pred[label_test==1]==1)/sum(label_test==1))\n",
    "        SPECIFICITY = np.append(SPECIFICITY, sum(y_pred[label_test==0]==0)/sum(label_test==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96d63ee-c6f9-46b8-97f1-1a968713fbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prining the results for tables\n",
    "np.save(f\"BA_Entropy\",AUC_TEST)\n",
    "np.save(f\"sens_Entropy\",SENSITIVITY)\n",
    "np.save(f\"spec_Entropy\",SPECIFICITY)\n",
    "Q_bar, STD = np.mean(AUC_TEST), np.std(AUC_TEST)\n",
    "z = 1.96\n",
    "result= stats.ttest_1samp(AUC_TEST, level, alternative=\"greater\")\n",
    "print(\"mean value of accuracy:\", np.round(Q_bar,2))\n",
    "print(\"standard deviation of accuracy:\", np.round(STD,2))\n",
    "print(\"Confidence interval:\", np.round(Q_bar-z*STD,2), np.round(Q_bar+z*STD,2))\n",
    "print(\"p value\",result.pvalue)\n",
    "print(\"Sensitivity:\",np.round(np.mean(SENSITIVITY),2))\n",
    "print(\"Specificity:\",np.round(np.mean(SPECIFICITY),2))\n",
    "print(\"optimal number of features for classification:\", N_OPT)\n",
    "print(\"mean value of optimal numbers:\",np.mean(N_OPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed8ef45-9c93-4d1f-b2a2-f83dc3abed42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
