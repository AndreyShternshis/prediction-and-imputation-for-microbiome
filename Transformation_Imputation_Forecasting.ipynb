{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c0788a1-3479-424f-a8ac-073cd7b3dfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###import python packages \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_selection import (f_classif,SelectKBest)\n",
    "from sklearn.preprocessing import normalize\n",
    "import scipy.stats as stats\n",
    "from scipy.cluster import hierarchy\n",
    "import warnings\n",
    "###import libraries for log-transformation and imputation\n",
    "import sys\n",
    "sys.path.insert(0, '/libraries')\n",
    "from libraries import Transformation_library, Imputation_library\n",
    "###fix random seed for reproducibility \n",
    "import os\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a830bb4-21fc-4e5d-8439-fbe1741ae58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###user-defined parameters\n",
    "is_balanced = 0 #do we want to use weights during classification? (as discussed in section 4.1)\n",
    "is_median = 0 #do we want to use median or mean when select optimal n (number of features, as discussed in section 4.1)\n",
    "Dataset_N = 2 #number of the dataset (1 stands for postpartum depression and 2 is for food allergies)\n",
    "Imputation_type = [\"No\", \"Imputation\", \"Oversampling\"][1] # imputation is discussed in section 3.4, oversamplng is applied in section 4.2)\n",
    "Imputation_by = [\"linear\", \"SVR\", \"GPR\", \"CVAE\", \"cGAN\"][4]  #if imputation (or oversampling) is applied, the method is chosen from the list in section 3.4\n",
    "Transoformation_type = [\"Compositional\",\"CLR\", \"ALR\", \"PLR\"][0] #Log-transfromations from section 3.3. This option is Dataset 2, because the features are fixed for Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4381a13b-e988-4510-ac4e-096a093260bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "###standard parameters\n",
    "level = 0.5 #balanced accuracy when predictions are made by random guess\n",
    "test = 0.2 #fraction of data used for test set\n",
    "val = 0.2 #fraction for validation (from the rest part)\n",
    "ITER = 5 #number of iteration for random forest (section 3.5)\n",
    "n_features = 20 #the maximum number of features when selecting by ANOVA (as discussed in section 3.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "041024d7-b701-486a-a880-ef69854a84d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sets = int(1/test) #number of testing sets (5)\n",
    "val_sets = int(1/val) #number of validation sets (5)\n",
    "if Dataset_N==1:\n",
    "    binary = 11 #threshold to be classified as helthy/depressed \n",
    "if Dataset_N==2:\n",
    "    is_sparse = 1 #1 if bactria are merged (as discussed in section 3.2)\n",
    "    sparsity_level = 0.9 #sparsity threshold to merge bacteria together (as discussed in section 3.2)\n",
    "if is_balanced == 1:\n",
    "    class_weight = \"balanced\"\n",
    "else:\n",
    "    class_weight = None\n",
    "if Imputation_type==\"No\" or Imputation_by in [\"linear\", \"SVR\"]: #number of imputation sets\n",
    "    Imputation_sets = 1\n",
    "else:\n",
    "    Imputation_sets = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18c63d70-9eb8-451e-86da-7157f149cf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read metadata file with id of participants and time points (depending on dataset number)\n",
    "if Dataset_N==1:\n",
    "    Metadata=pd.read_csv(\"BASIC_metadata_full.csv\",sep=',',low_memory=False)\n",
    "    Metadata.loc[Metadata.TimePoint==\"Trimester2\",\"TimePoint\"] = 0 #timepoits are 0,1,2\n",
    "    Metadata.loc[Metadata.TimePoint==\"Trimester3\",\"TimePoint\"] = 1\n",
    "    Metadata.loc[Metadata.TimePoint==\"PostpartumWeek6\",\"TimePoint\"] = 2\n",
    "    TARGET = 2*np.ones_like(Metadata.EPDS) #2 is for missingness\n",
    "    TARGET[Metadata.EPDS>binary] = 1 #binary labels for depression\n",
    "    TARGET[Metadata.EPDS<=binary] = 0\n",
    "if Dataset_N==2:\n",
    "    Metadata=pd.read_csv(\"GMAP_metadata_public.csv\",sep=\",\",low_memory=False) \n",
    "    Metadata = Metadata.rename(columns={\"sample\": \"tp\"})\n",
    "    Metadata.loc[Metadata.tp==\"3_twomonth\",\"tp\"] = 0\n",
    "    Metadata.loc[Metadata.tp==\"4_fourmonth\",\"tp\"] = 1\n",
    "    Metadata.loc[Metadata.tp==\"5_sixmonth\",\"tp\"] = 2\n",
    "    time = [\".3.twomonth\", \".4.fourmonth\", \".5.sixmonth\"]\n",
    "    i = Metadata[pd.Series.isna(Metadata.case_id)].index #remove NaN values\n",
    "    Metadata = Metadata.drop(i)\n",
    "    TARGET = np.zeros_like(Metadata.case_id)\n",
    "    TARGET[Metadata.case_id.to_numpy()==\"AP Case\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa6badf1-0fbf-4322-998b-63886f0ba5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read file with compositional data (depending on dataset number)\n",
    "if Dataset_N==1:\n",
    "    profile =pd.read_csv(\"Species_Profile_full.csv\",sep=',',low_memory=False)\n",
    "    species=profile.to_numpy()[:,1:]/100 # normalized to 1\n",
    "    full_list_bacteria = list(profile.columns)[1:]\n",
    "    Where = lambda x: np.where(np.array(full_list_bacteria) == x)[0][0]\n",
    "    arg_bacteria = [Where(\"Veillonella_parvula\"), Where(\"Haemophilus_parainfluenzae\")] #for Dataset 1 the bacteria space is predefined by species identified in Table 5\n",
    "if Dataset_N==2:\n",
    "    profile =pd.read_csv(\"feature-table-not-filtered-l6.csv\",sep=',',low_memory=False)\n",
    "    profile.index = profile['Sample_id']\n",
    "    profile = profile.drop([\"Unassigned;__;__;__;__;__\", \"k__Archaea;__;__;__;__;__\", \"k__Bacteria;__;__;__;__;__\"]) #not annonated bacteria are ommited\n",
    "    profile = profile.drop([\"Sample_id\"], axis=1)\n",
    "    profile = profile.transpose()\n",
    "    species=profile.to_numpy()[:,1:]\n",
    "    species = normalize(species, axis=1, norm='l1') #make the data compositional\n",
    "d_full = np.shape(species)[1] #number of bacteria before merging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e63400d3-e268-4d71-9b27-8984ef500c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "###make separate arrays for complete and incomplete data points (depending on dataset number)\n",
    "if Dataset_N==1:\n",
    "    X_full, X_0, X_1, labels, labels_0, labels_1 = [[] for _ in range(6)] #X is data in 2 rows, labels are EPDS at tp 2\n",
    "    ID = profile.Sample_id.to_numpy() #id of a sample in profile\n",
    "    Individual_ID = Metadata.Individual_ID #id of a person\n",
    "if Dataset_N==2:\n",
    "    X_full, X_01, X_02, X_12, labels, labels_01, labels_02, labels_12 = [[] for _ in range(8)] #X is data in 3 rows, labels are AP case at any tp\n",
    "    ID = profile.index #id of a sample in profile\n",
    "    Individual_ID = Metadata.family_id.to_numpy() #id of a person\n",
    "    for k in Metadata.index:\n",
    "        j = Metadata.tp[k]\n",
    "        if j in [0,1,2]: #iterating in 3 time points\n",
    "            i = Metadata.family_id[k]\n",
    "            tp_id = \"sub.\" + str(i) + time[j]\n",
    "            if 1 - (tp_id in ID):\n",
    "                Metadata.loc[k, \"tp\"] = np.nan #checking if microbiome is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7dc458f-9618-4496-b1ca-7c725cbde938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill in the arrays for complete and  incomplete datapoints (depending on dataset number)\n",
    "if Dataset_N==1:\n",
    "    for i in np.unique(Individual_ID):\n",
    "        TP = Metadata.TimePoint[Individual_ID == i].to_numpy()\n",
    "        if sum((TP - np.array([0,1,2]))**2)!=0:\n",
    "            warnings.warn(\"TP are missing or in incorrect order\") #does not happen in the dataset we have\n",
    "        Outcomes = TARGET[Individual_ID == i]\n",
    "        if Outcomes[2]!=2:\n",
    "            if Imputation_type == \"Oversampling\" and Outcomes[2]==0:\n",
    "                oversampling = 0\n",
    "            else:\n",
    "                oversampling = 1\n",
    "            Reads = Metadata.ReadsNumber[Individual_ID == i].to_numpy()\n",
    "            Sample_ID = Metadata.Sample_ID[Individual_ID == i] #id of a sample in metadata\n",
    "            if 1-np.isnan(Reads[0]) and 1-np.isnan(Reads[1]): #complete data\n",
    "                labels = np.append(labels, Outcomes[2])\n",
    "                Dataset_line = np.zeros((1,2,d_full))\n",
    "                for j in range(2):\n",
    "                    tp_id = Sample_ID[TP == j].to_numpy()\n",
    "                    Dataset_line[:, j, :] = species[np.where(ID == tp_id)[0], :]\n",
    "                X_full = Dataset_line if np.size(X_full) == 0 else np.concatenate([X_full, Dataset_line])\n",
    "            if 1-np.isnan(Reads[0]) and np.isnan(Reads[1]) and oversampling == 1: #tp 1 is missing\n",
    "                labels_0 = np.append(labels_0, Outcomes[2])\n",
    "                Dataset_line = np.zeros((1,2,d_full))\n",
    "                tp_id = Sample_ID[TP == 0].to_numpy()\n",
    "                Dataset_line[:, 0, :] = species[np.where(ID == tp_id)[0], :]\n",
    "                X_0 = Dataset_line if np.size(X_0) == 0 else np.concatenate([X_0, Dataset_line])\n",
    "            if np.isnan(Reads[0]) and 1-np.isnan(Reads[1]) and oversampling == 1: #tp 0 is missing\n",
    "                labels_1 = np.append(labels_1, Outcomes[2])\n",
    "                Dataset_line = np.zeros((1,2,d_full))\n",
    "                tp_id = Sample_ID[TP == 1].to_numpy()\n",
    "                Dataset_line[:, 1, :] = species[np.where(ID == tp_id)[0], :]\n",
    "                X_1 = Dataset_line if np.size(X_1) == 0 else np.concatenate([X_1, Dataset_line])\n",
    "if Dataset_N==2:\n",
    "    for i in np.unique(Individual_ID):\n",
    "        TP = Metadata.tp[Individual_ID == i].to_numpy()\n",
    "        Outcomes = TARGET[Individual_ID == i]\n",
    "        if np.mean(Outcomes)==Outcomes[-1]: #considering data points with unique tp\n",
    "            if Imputation_type == \"Oversampling\" and Outcomes[2]==0:\n",
    "                oversampling = 0\n",
    "            else:\n",
    "                oversampling = 1\n",
    "            if np.sum(TP == 0) == 1 and np.sum(TP == 1) == 1 and np.sum(TP == 2) == 1: #complete data\n",
    "                labels = np.append(labels, Outcomes[-1])\n",
    "                Dataset_line = np.zeros((1, 3, d_full))\n",
    "                for j in [0, 1, 2]:\n",
    "                    tp_id = \"sub.\" + str(i) + time[j]\n",
    "                    Dataset_line[:, j, :] = species[np.where(ID == tp_id)[0], :]\n",
    "                X_full = Dataset_line if np.size(X_full) == 0 else np.concatenate([X_full, Dataset_line])\n",
    "            if np.sum(TP == 0) == 1 and np.sum(TP == 1) == 1 and np.sum(TP == 2) == 0 and oversampling == 1: #tp2 is missing\n",
    "                labels_01 = np.append(labels_01, Outcomes[-1])\n",
    "                Dataset_line = np.zeros((1, 3, d_full))\n",
    "                for j in [0, 1]:\n",
    "                    tp_id = \"sub.\" + str(i) + time[j]\n",
    "                    Dataset_line[:, j, :] = species[np.where(ID == tp_id)[0], :]\n",
    "                X_01 = Dataset_line if np.size(X_01) == 0 else np.concatenate([X_01, Dataset_line])\n",
    "            if np.sum(TP == 0) == 1 and np.sum(TP == 1) == 0 and np.sum(TP == 2) == 1 and oversampling == 1: #tp1 is missing\n",
    "                labels_02 = np.append(labels_02, Outcomes[-1])\n",
    "                Dataset_line = np.zeros((1, 3, d_full))\n",
    "                for j in [0, 2]:\n",
    "                    tp_id = \"sub.\" + str(i) + time[j]\n",
    "                    Dataset_line[:, j, :] = species[np.where(ID == tp_id)[0], :]\n",
    "                X_02 = Dataset_line if np.size(X_02) == 0 else np.concatenate([X_02, Dataset_line])\n",
    "            if np.sum(TP == 0) == 0 and np.sum(TP == 1) == 1 and np.sum(TP == 2) == 1 and oversampling == 1: #tp0 is missing\n",
    "                labels_12 = np.append(labels_12, Outcomes[-1])\n",
    "                Dataset_line = np.zeros((1, 3, d_full))\n",
    "                for j in [1, 2]:\n",
    "                    tp_id = \"sub.\" + str(i) + time[j]\n",
    "                    Dataset_line[:, j, :] = species[np.where(ID == tp_id)[0], :]\n",
    "                X_12 = Dataset_line if np.size(X_12) == 0 else np.concatenate([X_12, Dataset_line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb12a242-842a-42f5-94a1-01ad0fc4651f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sizes of complete f and t sets: 30 20\n",
      "test group 0\n",
      "sizes of train sets 20 13\n",
      "val group 0\n",
      "d= 56\n",
      "val group 1\n",
      "d= 54\n",
      "val group 2\n",
      "d= 54\n",
      "val group 3\n",
      "d= 55\n",
      "val group 4\n",
      "d= 55\n",
      "means [0.68333333 0.645      0.69833333 0.70333333 0.755      0.725\n",
      " 0.73166667 0.775      0.73166667 0.755      0.71       0.665\n",
      " 0.69       0.625      0.645      0.645      0.63333333 0.63833333\n",
      " 0.61666667 0.61833333]\n",
      "test features: ['x19' 'x74' 'x120' 'x129' 'x137' 'x139' 'x151' 'x159']\n",
      "importances [0.13129212 0.16679762 0.07223918 0.16996665 0.12235095 0.10363565\n",
      " 0.14301834 0.0906995 ]\n",
      "auc 0.75\n",
      "importances [0.15628746 0.16872989 0.06790574 0.1694579  0.11704602 0.10945617\n",
      " 0.1379496  0.07316723]\n",
      "auc 0.75\n",
      "importances [0.13690985 0.17528316 0.08945864 0.18687286 0.13003851 0.08343732\n",
      " 0.12627276 0.07172691]\n",
      "auc 0.75\n",
      "importances [0.14045729 0.17073569 0.08263061 0.16157829 0.12947295 0.09602337\n",
      " 0.14697756 0.07212424]\n",
      "auc 0.75\n",
      "importances [0.14109915 0.16532437 0.0807093  0.16079556 0.13312728 0.10233763\n",
      " 0.14660381 0.0700029 ]\n",
      "auc 0.75\n",
      "test group 1\n",
      "sizes of train sets 20 13\n",
      "val group 0\n",
      "d= 56\n",
      "val group 1\n",
      "d= 58\n",
      "val group 2\n",
      "d= 58\n",
      "val group 3\n",
      "d= 58\n",
      "val group 4\n",
      "d= 57\n",
      "means [0.73333333 0.62333333 0.775      0.76166667 0.69833333 0.77833333\n",
      " 0.78333333 0.74666667 0.75333333 0.73333333 0.715      0.73833333\n",
      " 0.7        0.68166667 0.67333333 0.65833333 0.66166667 0.69666667\n",
      " 0.64166667 0.64333333]\n",
      "test features: ['x19' 'x76' 'x133' 'x142' 'x144' 'x156' 'x172']\n",
      "importances [0.171093   0.16952438 0.15625755 0.12469693 0.13015907 0.11928048\n",
      " 0.12898859]\n",
      "auc 0.7083333333333334\n",
      "importances [0.17760761 0.17717318 0.16159375 0.12229978 0.14312703 0.11234912\n",
      " 0.10584954]\n",
      "auc 0.7083333333333334\n",
      "importances [0.16676705 0.179342   0.13953188 0.1432359  0.12659421 0.12301004\n",
      " 0.12151893]\n",
      "auc 0.7083333333333334\n",
      "importances [0.16985692 0.16207835 0.14505689 0.13564703 0.1335915  0.13088501\n",
      " 0.1228843 ]\n",
      "auc 0.7083333333333334\n",
      "importances [0.16941497 0.15940803 0.15803203 0.12903881 0.14942654 0.12705112\n",
      " 0.1076285 ]\n",
      "auc 0.7083333333333334\n",
      "test group 2\n",
      "sizes of train sets 20 13\n",
      "val group 0\n",
      "d= 54\n",
      "val group 1\n",
      "d= 58\n",
      "val group 2\n",
      "d= 57\n",
      "val group 3\n",
      "d= 57\n",
      "val group 4\n",
      "d= 57\n",
      "means [0.61666667 0.495      0.59833333 0.64833333 0.67833333 0.69166667\n",
      " 0.67333333 0.65666667 0.65333333 0.64666667 0.57       0.54666667\n",
      " 0.55666667 0.57       0.615      0.66666667 0.55       0.54333333\n",
      " 0.655      0.60166667]\n",
      "test features: ['x19' 'x27' 'x76' 'x133' 'x142' 'x156']\n",
      "importances [0.1606086  0.13101608 0.1687091  0.21408358 0.13536867 0.19021396]\n",
      "auc 0.4583333333333334\n",
      "importances [0.16928861 0.13011076 0.15513809 0.24193265 0.14104091 0.16248897]\n",
      "auc 0.4583333333333334\n",
      "importances [0.16635638 0.13076074 0.15999498 0.22515702 0.14283594 0.17489494]\n",
      "auc 0.375\n",
      "importances [0.1677874  0.1341597  0.15277394 0.2329167  0.13722779 0.17513447]\n",
      "auc 0.375\n",
      "importances [0.16233754 0.13062337 0.1599715  0.24596704 0.14785466 0.15324588]\n",
      "auc 0.5833333333333334\n",
      "test group 3\n",
      "sizes of train sets 20 13\n",
      "val group 0\n",
      "d= 55\n",
      "val group 1\n",
      "d= 57\n",
      "val group 2\n",
      "d= 58\n",
      "val group 3\n",
      "d= 57\n",
      "val group 4\n",
      "d= 58\n",
      "means [0.61666667 0.60666667 0.63666667 0.59833333 0.63166667 0.715\n",
      " 0.725      0.71       0.74       0.68333333 0.63333333 0.63833333\n",
      " 0.61833333 0.64166667 0.61333333 0.63666667 0.61       0.615\n",
      " 0.60166667 0.63      ]\n",
      "test features: ['x19' 'x27' 'x77' 'x106' 'x135' 'x144' 'x146' 'x147' 'x151']\n",
      "importances [0.11911434 0.08146121 0.11988909 0.11693829 0.18912489 0.08252986\n",
      " 0.11622717 0.07889053 0.0958246 ]\n",
      "auc 0.5416666666666667\n",
      "importances [0.08939448 0.07132538 0.12250097 0.12327828 0.18356854 0.09815523\n",
      " 0.10644787 0.09919175 0.10613751]\n",
      "auc 0.7083333333333334\n",
      "importances [0.09380198 0.08150653 0.11976625 0.13666295 0.17240113 0.09928675\n",
      " 0.10958694 0.09382426 0.09316321]\n",
      "auc 0.7916666666666667\n",
      "importances [0.11304517 0.08021892 0.1208548  0.12323635 0.15461405 0.10512156\n",
      " 0.10714011 0.09034983 0.10541921]\n",
      "auc 0.7916666666666667\n",
      "importances [0.09889283 0.0920931  0.11719432 0.11949657 0.17210884 0.10876766\n",
      " 0.11634202 0.0975781  0.07752656]\n",
      "auc 0.7916666666666667\n",
      "test group 4\n",
      "sizes of train sets 20 13\n",
      "val group 0\n",
      "d= 55\n",
      "val group 1\n",
      "d= 56\n",
      "val group 2\n",
      "d= 57\n",
      "val group 3\n",
      "d= 56\n",
      "val group 4\n",
      "d= 57\n",
      "means [0.61666667 0.70333333 0.68166667 0.69166667 0.72833333 0.70333333\n",
      " 0.72666667 0.715      0.73833333 0.7        0.695      0.65666667\n",
      " 0.64666667 0.64       0.58833333 0.62333333 0.59666667 0.57166667\n",
      " 0.55833333 0.55166667]\n",
      "test features: ['x19' 'x27' 'x76' 'x133' 'x142' 'x144' 'x156' 'x162' 'x167']\n",
      "importances [0.13809199 0.07582022 0.12294257 0.15961246 0.08417256 0.09198924\n",
      " 0.13634833 0.08272291 0.10829971]\n",
      "auc 0.7083333333333334\n",
      "importances [0.1393162  0.09734382 0.1284387  0.16504169 0.08002309 0.09027996\n",
      " 0.11409813 0.08506095 0.10039747]\n",
      "auc 0.7916666666666667\n",
      "importances [0.12707881 0.09934214 0.12128161 0.16471572 0.09085822 0.10419805\n",
      " 0.11836582 0.07721924 0.09694039]\n",
      "auc 0.7083333333333334\n",
      "importances [0.14433662 0.09260121 0.1184858  0.15911112 0.08586288 0.0805665\n",
      " 0.1231849  0.09100303 0.10484795]\n",
      "auc 0.7083333333333334\n",
      "importances [0.13618488 0.08186645 0.13754785 0.17368131 0.08484154 0.08358123\n",
      " 0.12215553 0.08575782 0.09438339]\n",
      "auc 0.7083333333333334\n",
      "test group 0\n",
      "sizes of train sets 20 13\n",
      "val group 0\n",
      "d= 56\n",
      "val group 1\n",
      "d= 54\n",
      "val group 2\n",
      "d= 54\n",
      "val group 3\n",
      "d= 55\n",
      "val group 4\n",
      "d= 55\n",
      "means [0.7        0.63       0.71166667 0.675      0.68833333 0.70166667\n",
      " 0.7        0.72166667 0.68666667 0.75666667 0.735      0.67833333\n",
      " 0.63833333 0.585      0.615      0.605      0.59833333 0.605\n",
      " 0.60833333 0.61166667]\n",
      "test features: ['x19' 'x26' 'x74' 'x96' 'x129' 'x132' 'x137' 'x139' 'x151' 'x162']\n",
      "importances [0.12772723 0.07527418 0.12041464 0.1161524  0.1478413  0.05476992\n",
      " 0.08040313 0.09602462 0.10210057 0.07929201]\n",
      "auc 0.75\n",
      "importances [0.11819396 0.08497749 0.13174845 0.09878976 0.14864727 0.05547541\n",
      " 0.09340179 0.07757704 0.1088363  0.08235251]\n",
      "auc 0.75\n",
      "importances [0.13163116 0.06949763 0.12796597 0.10274386 0.14785124 0.05532146\n",
      " 0.08392428 0.08248258 0.10059682 0.09798499]\n",
      "auc 0.75\n",
      "importances [0.14370423 0.0740905  0.11013304 0.09396876 0.14981225 0.05318193\n",
      " 0.0761925  0.09381151 0.10940819 0.09569709]\n",
      "auc 0.75\n",
      "importances [0.14177906 0.07316721 0.11990735 0.09835547 0.14774491 0.05974775\n",
      " 0.0931869  0.08558647 0.09748403 0.08304084]\n",
      "auc 0.625\n",
      "test group 1\n",
      "sizes of train sets 20 13\n",
      "val group 0\n",
      "d= 56\n",
      "val group 1\n",
      "d= 58\n",
      "val group 2\n",
      "d= 58\n",
      "val group 3\n",
      "d= 58\n",
      "val group 4\n",
      "d= 57\n",
      "means [0.7        0.56333333 0.70333333 0.69666667 0.73       0.78166667\n",
      " 0.745      0.76666667 0.72166667 0.73       0.69666667 0.72166667\n",
      " 0.71166667 0.69833333 0.7        0.69166667 0.68166667 0.675\n",
      " 0.72166667 0.68666667]\n",
      "test features: ['x19' 'x76' 'x133' 'x142' 'x144' 'x172']\n",
      "importances [0.15999064 0.22204818 0.15896225 0.16476985 0.14185057 0.1523785 ]\n",
      "auc 0.7083333333333334\n",
      "importances [0.15512246 0.2103359  0.16807263 0.1874807  0.13361961 0.1453687 ]\n",
      "auc 0.7083333333333334\n",
      "importances [0.17141124 0.19684568 0.16642772 0.18032918 0.14039304 0.14459315]\n",
      "auc 0.7083333333333334\n",
      "importances [0.17258333 0.20837193 0.17330911 0.18729332 0.12820627 0.13023603]\n",
      "auc 0.7083333333333334\n",
      "importances [0.17364355 0.19120535 0.16500485 0.18839125 0.1391687  0.1425863 ]\n",
      "auc 0.7083333333333334\n",
      "test group 2\n",
      "sizes of train sets 20 13\n",
      "val group 0\n",
      "d= 54\n",
      "val group 1\n",
      "d= 58\n",
      "val group 2\n",
      "d= 57\n",
      "val group 3\n",
      "d= 57\n"
     ]
    }
   ],
   "source": [
    "###imputation, transformation, and classification of data into for-loops for imputation, test, and validation sets\n",
    "X_f, X_t = X_full[labels == 0], X_full[labels == 1] #f = healthy, t = depressed/allegic\n",
    "size_f, size_t = np.shape(X_f)[0], np.shape(X_t)[0]\n",
    "print(\"sizes of complete sets:\",size_f,size_t)\n",
    "test_f, test_t = int(size_f*test), int(size_t*test) #sizes of test sets\n",
    "AUC_TEST, N_OPT, SENSITIVITY, SPECIFICITY = [[] for _ in range(4)]\n",
    "for IS in range(Imputation_sets): #iterating over imputation sets since the imputed values can be random\n",
    "    sample_seed = IS + ITER\n",
    "    aucs_val = np.zeros((val_sets, ITER, n_features))\n",
    "    for TS in range(test_sets): #iterating over test sets\n",
    "        print(\"test group\", TS)\n",
    "        test_f_range, test_t_range = range(TS * test_f, (TS + 1) * test_f), range(TS * test_t, (TS + 1) * test_t)\n",
    "        rest_f_range, rest_t_range = np.setdiff1d(range(size_f), test_f_range), np.setdiff1d(range(size_t), test_t_range)\n",
    "        X_f_test, X_t_test = X_f[test_f_range], X_t[test_t_range]\n",
    "        X_test = np.concatenate([X_f_test,X_t_test])\n",
    "        label_test = np.zeros(test_f + test_t)\n",
    "        label_test[test_f:] = 1\n",
    "        rest_f, rest_t = np.size(rest_f_range), np.size(rest_t_range)\n",
    "        val_f, val_t = int(rest_f * val), int(rest_t * val)  #sizes of validation sets\n",
    "        train_f, train_t = rest_f - val_f, rest_t - val_t #the rest of rest is for training set\n",
    "        print(\"sizes of train sets\", train_f, train_t)\n",
    "        for VS in range(val_sets): #iterating over vaidation sets\n",
    "            print(\"val group\", VS)\n",
    "            val_f_range, val_t_range = rest_f_range[range(VS* val_f, (VS + 1) * val_f)], rest_t_range[range(VS* val_t, (VS + 1) * val_t)]\n",
    "            train_f_range, train_t_range = np.setdiff1d(rest_f_range, val_f_range), np.setdiff1d(rest_t_range, val_t_range)\n",
    "            X_f_val, X_t_val, X_f_train, X_t_train = X_f[val_f_range], X_t[val_t_range], X_f[train_f_range], X_t[train_t_range]\n",
    "            X_val, X_train = np.concatenate([X_f_val, X_t_val]), np.concatenate([X_f_train, X_t_train])\n",
    "            label_val, label_train = np.zeros(val_f + val_t), np.zeros(train_f + train_t)\n",
    "            label_val[val_f:], label_train[train_f:] = 1, 1\n",
    "            if Dataset_N == 1:\n",
    "                X_test_cut, X_0_cut, X_1_cut = X_test, X_0, X_1\n",
    "                X = [X_train, X_val, X_test_cut, X_0_cut, X_1_cut]\n",
    "                d = np.shape(X[0])[2]\n",
    "                delta = np.min(X[0][X[0] > 0]) # the smallest non-negative element used to replace 0 values (as discussed in section 3.3)\n",
    "                for xi in range(len(X)):\n",
    "                    X[xi] = (X[xi] + delta) / (1 + delta * d) #remove 0s\n",
    "                X_train, X_val, X_test_cut, X_0_cut, X_1_cut = X\n",
    "                if Imputation_type!=\"No\":\n",
    "                    ###Classes show what data were originally missing. They are the part of the input for classifier\n",
    "                    classes_train, classes_val, classes_test = np.ones((np.shape(X_train)[0],2)),np.ones((np.shape(X_val)[0],2)),np.ones((np.shape(X_test_cut)[0],2))\n",
    "                    classes_0 = np.ones((np.shape(X_0_cut)[0], 2))\n",
    "                    classes_1 = np.ones((np.shape(X_1_cut)[0], 2))\n",
    "                    classes_0[:,1]=0\n",
    "                    classes_1[:,0]=0\n",
    "                    ###Imputation is done as descrbed in corresponding section 3.5\n",
    "                    X_1_cut = Imputation_library.impute(X_train, X_val, X_1_cut, 1, 0, sample_seed, arg_bacteria, Imputation_by)\n",
    "                    X_0_cut = Imputation_library.impute(X_train, X_val, X_0_cut, 0, 1, sample_seed, arg_bacteria, Imputation_by)\n",
    "                    ### concatenate full set with imputed sets\n",
    "                    X_train = np.concatenate([X_train, X_0_cut, X_1_cut])\n",
    "                    classes_train = np.concatenate([classes_train, classes_0,classes_1])\n",
    "                    label_train = np.concatenate([label_train, labels_0, labels_1])\n",
    "                    ### construct features from 2 bacteria\n",
    "                stats_train, stats_val = Transformation_library.transform_2(X_train,  arg_bacteria), Transformation_library.transform_2(X_val, arg_bacteria)\n",
    "            if Dataset_N == 2:\n",
    "                ###bacteria are merged if decided by a user\n",
    "                X_test_cut, X_01_cut, X_02_cut, X_12_cut = X_test, X_01, X_02, X_12 #no changes in data dimensionality if is_sparse = False\n",
    "                X = [X_train, X_val, X_test_cut, X_01_cut, X_02_cut, X_12_cut]\n",
    "                if is_sparse == 1:\n",
    "                    if Imputation_type==\"No\":\n",
    "                        X_combine = X_train.reshape(-1,d_full)\n",
    "                    else:\n",
    "                        X_combine = np.concatenate([X_train.reshape(-1,d_full), X_01_cut[:,[0,1],:].reshape(-1,d_full),X_02_cut[:,[0,2],:].reshape(-1,d_full),X_12_cut[:,[1,2],:].reshape(-1,d_full)])\n",
    "                    sparsity = np.sum(X_combine == 0, axis=0) / np.shape(X_combine)[0]\n",
    "                    for xi in range(len(X)):\n",
    "                        other_species = np.sum(X[xi][:, :, sparsity >= sparsity_level], axis=-1).reshape(-1, 3, 1)\n",
    "                        X[xi] = X[xi][:, :, sparsity < sparsity_level]\n",
    "                        X[xi]= np.concatenate([X[xi], other_species], axis=-1)\n",
    "                d = np.shape(X[0])[2]\n",
    "                print(\"d=\", d)\n",
    "                delta = np.min(X[0][X[0] > 0]) # the smallest non-negative element used to replace 0 values (as discussed in section 3.3)\n",
    "                for xi in range(len(X)):\n",
    "                    X[xi] = (X[xi] + delta) / (1 + delta * d) #remove 0s\n",
    "                X_train, X_val, X_test_cut, X_01_cut, X_02_cut, X_12_cut = X\n",
    "                if Imputation_type!=\"No\":\n",
    "                    ###Classes show what data were originally missing. They are the part of the input for classifier\n",
    "                    classes_train, classes_val, classes_test = np.ones((np.shape(X_train)[0], 3)), np.ones((np.shape(X_val)[0], 3)), np.ones((np.shape(X_test_cut)[0], 3))\n",
    "                    classes_01, classes_02, classes_12 = np.ones((np.shape(X_01_cut)[0], 3)), np.ones((np.shape(X_02_cut)[0], 3)), np.ones((np.shape(X_12_cut)[0], 3))\n",
    "                    classes_01[:,2], classes_02[:,1], classes_12[:,0] = 0,0,0\n",
    "                    ###Imputation is done as descrbed in corresponding section 3.5\n",
    "                    X_01_cut = Imputation_library.impute_one(X_train, X_val, X_01_cut, [0, 1], 2, sample_seed, Imputation_by)\n",
    "                    X_02_cut = Imputation_library.impute_one(X_train, X_val, X_02_cut, [0, 2], 1, sample_seed, Imputation_by)\n",
    "                    X_12_cut = Imputation_library.impute_one(X_train, X_val, X_12_cut, [1, 2], 0, sample_seed, Imputation_by)\n",
    "                    ### concatenate full set with imputed sets\n",
    "                    X_train = np.concatenate([X_train, X_01_cut, X_02_cut, X_12_cut])\n",
    "                    classes_train = np.concatenate([classes_train, classes_01, classes_02, classes_12])\n",
    "                    label_train = np.concatenate([label_train, labels_01, labels_02, labels_12])\n",
    "                dY = 3 * d #the dimensionality is 3 time points multipled by the number of bacteria\n",
    "                X_train, X_val = np.reshape(X_train, (-1, dY)) / 3, np.reshape(X_val, (-1, dY)) / 3  # flatten inputs to one compositional vector\n",
    "                ### log-transformations from setion 3.3\n",
    "                if Transoformation_type == \"CLR\":\n",
    "                    stats_train, stats_val = Transformation_library.CLR(X_train), Transformation_library.CLR(X_val)\n",
    "                elif Transoformation_type == \"ALR\":\n",
    "                    stats_train, stats_val = Transformation_library.LR(X_train), Transformation_library.LR(X_val)\n",
    "                elif Transoformation_type == \"PLR\":\n",
    "                    ### construct hierarchy tree as in Figure 1\n",
    "                    Y_train = Transformation_library.CLR(X_train)\n",
    "                    Z = hierarchy.linkage(np.transpose(Y_train), method=\"ward\")\n",
    "                    clustersize, clusters = Transformation_library.hierarchy_tree(Z, dY)\n",
    "                    ### construct features from hierarchy tree\n",
    "                    stats_train, stats_val = Transformation_library.data_transformation(X_train, Z, clustersize, clusters,dY), Transformation_library.data_transformation(X_val, Z, clustersize, clusters, dY) \n",
    "                else:\n",
    "                    stats_train, stats_val = X_train, X_val\n",
    "            if Imputation_type==\"Imputation\":  \n",
    "                stats_train, stats_val = np.concatenate([classes_train, stats_train], axis=-1), np.concatenate([classes_val, stats_val], axis=-1)\n",
    "            n_features = np.min([n_features, np.shape(stats_train)[1]])\n",
    "            for j in range(n_features): #choose the best j + 1 features using ANOVA (section 3.2)\n",
    "                sel = SelectKBest(f_classif, k=j + 1).fit(pd.DataFrame(stats_train), label_train)\n",
    "                Index = sel.get_support()\n",
    "                for seed in range(ITER): #classify by random forest with different random seeds (section 3.4)\n",
    "                    rf = RandomForestClassifier(random_state = seed, class_weight=class_weight).fit(stats_train[:,Index], label_train)\n",
    "                    y_pred = rf.predict(stats_val[:,Index])\n",
    "                    aucs_val[VS, seed, j] = roc_auc_score(label_val, y_pred)\n",
    "        if is_median == 1: #optimal number of features by median of mean\n",
    "            print(\"medians\", np.median(aucs_val, axis=(0, 1))) #average by validation sets and random seeds\n",
    "            n_opt = np.argmax(np.median(aucs_val, axis=(0, 1))) + 1\n",
    "        else:\n",
    "            print(\"means\", np.mean(aucs_val, axis = (0,1)))\n",
    "            n_opt=np.argmax(np.mean(aucs_val, axis = (0,1))) + 1\n",
    "        N_OPT =np.append(N_OPT, n_opt)\n",
    "        X_rest, label_rest = np.concatenate([X_train, X_val]), np.concatenate([label_train,label_val]) #concatenate data except testing set\n",
    "        if Dataset_N==1:\n",
    "            ### construct features from 2 bacteria\n",
    "            stats_rest, stats_test = Transformation_library.transform_2(X_rest,  arg_bacteria), Transformation_library.transform_2(X_test_cut,  arg_bacteria)\n",
    "        if Dataset_N==2:\n",
    "            X_test_cut = np.reshape(X_test_cut, (-1, dY)) / 3 # flatten test samples to one compositional vector\n",
    "            ### log-transformations from setion 3.3\n",
    "            if Transoformation_type == \"CLR\":\n",
    "                stats_rest, stats_test = Transformation_library.CLR(X_rest), Transformation_library.CLR(X_test_cut)\n",
    "            elif Transoformation_type == \"ALR\":\n",
    "                stats_rest, stats_test = Transformation_library.LR(X_rest), Transformation_library.LR(X_test_cut)\n",
    "            elif Transoformation_type == \"PLR\":\n",
    "                ### construct hierarchy tree\n",
    "                Y_rest = Transformation_library.CLR(X_rest)\n",
    "                Z = hierarchy.linkage(np.transpose(Y_rest), method=\"ward\")\n",
    "                clustersize, clusters = Transformation_library.hierarchy_tree(Z, dY)\n",
    "                ### construct features from hierarchy tree \n",
    "                stats_rest, stats_test = Transformation_library.data_transformation(X_rest, Z, clustersize, clusters,dY), Transformation_library.data_transformation(X_test_cut, Z, clustersize, clusters, dY) \n",
    "            else:\n",
    "                stats_rest, stats_test = X_rest, X_test_cut\n",
    "        if Imputation_type==\"Imputation\":\n",
    "            classes_rest = np.concatenate([classes_train,classes_val]) #concatenate classes except testing set\n",
    "            stats_rest, stats_test = np.concatenate([classes_rest, stats_rest], axis= -1), np.concatenate([classes_test, stats_test], axis= -1)\n",
    "        sel = SelectKBest(f_classif, k=n_opt).fit(pd.DataFrame(stats_rest), label_rest) #select optimal number of features\n",
    "        Index = sel.get_support()\n",
    "        print(\"test features:\",sel.get_feature_names_out()) #from features names we can reconstruct what bacteria we use\n",
    "        for seed in range(ITER): #iterations for random forest on test sets\n",
    "            rf = RandomForestClassifier(random_state = seed, class_weight=class_weight).fit(stats_rest[:, Index], label_rest)\n",
    "            print(\"importances\",rf.feature_importances_) #display importances of features\n",
    "            y_pred = rf.predict(stats_test[:, Index])\n",
    "            print(\"auc\", roc_auc_score(label_test, y_pred)) #display balanced accuracy\n",
    "            AUC_TEST = np.append(AUC_TEST, roc_auc_score(label_test, y_pred)) # balanced accuracy on testing set\n",
    "            SENSITIVITY = np.append(SENSITIVITY, sum(y_pred[label_test==1]==1)/sum(label_test==1))\n",
    "            SPECIFICITY = np.append(SPECIFICITY, sum(y_pred[label_test==0]==0)/sum(label_test==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85387bb6-c0a1-46e9-9d26-c0caaeae4253",
   "metadata": {},
   "outputs": [],
   "source": [
    "#correction of variance by Rubin's rule with multiple imputations\n",
    "def rubins_rules(accuracies, n):\n",
    "    accuracies = accuracies.reshape(-1,n)\n",
    "    estimates = np.mean(accuracies, axis=1)\n",
    "    variances = np.var(accuracies,axis=1)\n",
    "    M = len(estimates)\n",
    "    Q_bar = np.mean(estimates)\n",
    "    U_bar = np.mean(variances)\n",
    "    B = np.sum((estimates - Q_bar)**2) / (M - 1)\n",
    "    T = U_bar + (1 + 1/M) * B\n",
    "    SE = np.sqrt(T)\n",
    "    z = 1.96\n",
    "    r = (1+1/M)*B/U_bar\n",
    "    nu = (M-1)*(1+r**(-1))**2\n",
    "    return Q_bar, T, Q_bar - z * SE, Q_bar + z * SE, nu\n",
    "#save data for the comparison between approaches \n",
    "np.save(f\"BA_{is_balanced}_{is_median}_{Dataset_N}_{Imputation_type}_{Imputation_by}_{Transoformation_type}\",AUC_TEST)\n",
    "np.save(f\"sens_{is_balanced}_{is_median}_{Dataset_N}_{Imputation_type}_{Imputation_by}_{Transoformation_type}\",SENSITIVITY)\n",
    "np.save(f\"spec_{is_balanced}_{is_median}_{Dataset_N}_{Imputation_type}_{Imputation_by}_{Transoformation_type}\",SPECIFICITY)\n",
    "#prining the results for tables\n",
    "if Imputation_sets == 1:\n",
    "    Q_bar, STD = np.mean(AUC_TEST), np.std(AUC_TEST)\n",
    "    z = 1.96\n",
    "    print(np.round(Q_bar,2), np.round(STD,2), np.round(Q_bar-z*STD,2), np.round(Q_bar+z*STD,2))\n",
    "    result= stats.ttest_1samp(AUC_TEST, level, alternative=\"greater\")\n",
    "    print(\"p value\",result.pvalue)\n",
    "else:\n",
    "    Q_bar, T, LB, UB, nu = rubins_rules(AUC_TEST,ITER)\n",
    "    print(np.round(Q_bar,2), np.round(np.sqrt(T),2), np.round(LB,2), np.round(UB,2))\n",
    "    W = (Q_bar - level)/T\n",
    "    print(W)\n",
    "    pvalue = stats.f.sf(W,1,nu)\n",
    "    print(\"p value\", pvalue)\n",
    "print(\"true positive\",np.round(np.mean(SENSITIVITY),2))\n",
    "print(\"true negative\",np.round(np.mean(SPECIFICITY),2))\n",
    "print(\"n opt\",np.mean(N_OPT))\n",
    "print(N_OPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3beac3ce-830e-42fd-bce9-fa58c3f832ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
