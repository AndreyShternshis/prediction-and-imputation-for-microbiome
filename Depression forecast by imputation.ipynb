{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba37fc1-d017-4477-b950-aeeab02c8edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "###import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_selection import (f_classif,SelectKBest)\n",
    "import scipy.stats as stats\n",
    "import warnings\n",
    "import sys\n",
    "sys.path.insert(0, '/libraries')\n",
    "from libraries import Imputation_library\n",
    "import os\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300c812b-5b11-4dbb-b948-57d7b8b35bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "###user-defined parameters\n",
    "is_balanced = 0 #do we want to use weights during classification?\n",
    "is_median = 1 #do we want to use median or mean when select optimal n (number of features)\n",
    "Dataset_N = 1 #number of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3289115-8d36-400d-be80-67e7b6ee63b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "###standard parameters\n",
    "level = 0.5 #auc when guess is random\n",
    "test = 0.2 #fraction for testing\n",
    "val = 0.2 #fraction for validation (from the rest part)\n",
    "ITER = 5 #number of iteration for random forest\n",
    "Imputation_sets = 5 #number of imputation sets\n",
    "n_features = 20 #the maximum number of features we consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d03ac16-0574-481b-8f65-a0838e9a28d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sets = int(1/test) #number of testing sets (5)\n",
    "val_sets = int(1/val) #number of validation sets (5)\n",
    "if Dataset_N==1:\n",
    "    binary = 11 #threshold for depression\n",
    "if Dataset_N==2:\n",
    "    is_sparse = 1 #do we want to reduce sparsity?\n",
    "    sparsity_level = 0.9 #sparsity threshold to merge bacteria together\n",
    "if is_balanced == 1:\n",
    "    class_weight = \"balanced\"\n",
    "else:\n",
    "    class_weight = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14329601-d800-4b0b-875e-1ce5d2ed67d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###metadata\n",
    "if Dataset_N==1:\n",
    "    Metadata=pd.read_csv(\"BASIC_metadata_full.csv\",sep=',',low_memory=False) #read file with depression levels and ids of participants\n",
    "    Metadata.loc[Metadata.TimePoint==\"Trimester2\",\"TimePoint\"] = 0 #timepoits are 0,1,2\n",
    "    Metadata.loc[Metadata.TimePoint==\"Trimester3\",\"TimePoint\"] = 1\n",
    "    Metadata.loc[Metadata.TimePoint==\"PostpartumWeek6\",\"TimePoint\"] = 2\n",
    "    i = Metadata[Metadata.ReadsNumber<500000].index #remove insufficient reads\n",
    "    #Metadata.loc[i, 'ReadsNumber'] = np.nan\n",
    "    EPDS = 2*np.ones_like(Metadata.EPDS) #2 is for missingness\n",
    "    EPDS[Metadata.EPDS>binary] = 1 #binary labels for depression\n",
    "    EPDS[Metadata.EPDS<=binary] = 0\n",
    "if Dataset_N==2:\n",
    "    Metadata=pd.read_csv(\"GMAP_metadata_public.csv\",sep=\",\",low_memory=False)\n",
    "    Metadata = Metadata.rename(columns={\"sample\": \"tp\"})\n",
    "    Metadata.loc[Metadata.tp==\"3_twomonth\",\"tp\"] = 0\n",
    "    Metadata.loc[Metadata.tp==\"4_fourmonth\",\"tp\"] = 1\n",
    "    Metadata.loc[Metadata.tp==\"5_sixmonth\",\"tp\"] = 2\n",
    "    time = [\".3.twomonth\", \".4.fourmonth\", \".5.sixmonth\"]\n",
    "    i = Metadata[pd.Series.isna(Metadata.case_id)].index #remove NaN values\n",
    "    Metadata = Metadata.drop(i)\n",
    "    AP = np.zeros_like(Metadata.case_id)\n",
    "    AP[Metadata.case_id.to_numpy()==\"AP Case\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad38cb2-8e9c-4a65-8c4e-60ca1d6b60a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###data\n",
    "profile =pd.read_csv(\"Species_Profile_full.csv\",sep=',',low_memory=False) #read file with compositional data\n",
    "species=profile.to_numpy()[:,1:]/100 #normilized to 1\n",
    "full_list_bacteria = list(profile.columns)[1:]\n",
    "Where = lambda x: np.where(np.array(full_list_bacteria) == x)[0][0]\n",
    "arg_bacteria = [Where(\"Veillonella_parvula\"), Where(\"Haemophilus_parainfluenzae\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a62b90e-7109-44cf-ac3f-77e558df8931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/opt/anaconda3/envs/imputation/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/anaconda3/envs/imputation/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/anaconda3/envs/imputation/lib/python3.9/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/anaconda3/envs/imputation/lib/python3.9/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/anaconda3/envs/imputation/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/anaconda3/envs/imputation/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/anaconda3/envs/imputation/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/anaconda3/envs/imputation/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/anaconda3/envs/imputation/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/anaconda3/envs/imputation/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/anaconda3/envs/imputation/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/anaconda3/envs/imputation/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/anaconda3/envs/imputation/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/opt/anaconda3/envs/imputation/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/anaconda3/envs/imputation/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/anaconda3/envs/imputation/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/imputation/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/anaconda3/envs/imputation/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/anaconda3/envs/imputation/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/anaconda3/envs/imputation/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/anaconda3/envs/imputation/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/anaconda3/envs/imputation/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/zs/rn85twmn2c72bhcjllkhmlnm0000gp/T/ipykernel_49097/312547253.py\", line 15, in <module>\n",
      "    from libraries import Imputation_library\n",
      "  File \"/Users/andsh459/PycharmProjects/Imputation for compositional data/libraries/Imputation_library.py\", line 5, in <module>\n",
      "    import torch\n",
      "  File \"/opt/anaconda3/envs/imputation/lib/python3.9/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/opt/anaconda3/envs/imputation/lib/python3.9/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/opt/anaconda3/envs/imputation/lib/python3.9/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/opt/anaconda3/envs/imputation/lib/python3.9/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/opt/anaconda3/envs/imputation/lib/python3.9/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/opt/anaconda3/envs/imputation/lib/python3.9/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sizes of complete f and t sets: 190 31\n",
      "sizes of test sets 38 6\n",
      "test group 0\n",
      "sizes of val sets 30 5\n",
      "sizes of train sets 122 20\n",
      "val group 0\n",
      "delta 5e-07\n",
      "val group 1\n",
      "delta 5e-07\n",
      "val group 2\n",
      "delta 5e-07\n",
      "val group 3\n",
      "delta 5e-07\n",
      "val group 4\n",
      "delta 5e-07\n",
      "medians [0.45       0.58333333 0.58333333]\n",
      "test features: ['x0' 'x1']\n",
      "importances [0.49928245 0.50071755]\n",
      "auc 0.4605263157894737\n",
      "importances [0.49088107 0.50911893]\n",
      "auc 0.4736842105263158\n",
      "importances [0.4930051 0.5069949]\n",
      "auc 0.4473684210526316\n",
      "importances [0.49791781 0.50208219]\n",
      "auc 0.4605263157894737\n",
      "importances [0.49725636 0.50274364]\n",
      "auc 0.4605263157894737\n",
      "test group 1\n",
      "sizes of val sets 30 5\n",
      "sizes of train sets 122 20\n",
      "val group 0\n",
      "delta 4.0000000000000003e-07\n",
      "val group 1\n",
      "delta 4.0000000000000003e-07\n",
      "val group 2\n",
      "delta 4.0000000000000003e-07\n",
      "val group 3\n",
      "delta 4.0000000000000003e-07\n",
      "val group 4\n",
      "delta 4.0000000000000003e-07\n",
      "medians [0.48333333 0.48333333 0.58333333]\n",
      "test features: ['x0' 'x1' 'x2']\n",
      "importances [0.49106852 0.4964613  0.01247018]\n",
      "auc 0.7368421052631579\n",
      "importances [0.47607428 0.50800332 0.01592239]\n",
      "auc 0.7368421052631579\n",
      "importances [0.50678097 0.48277046 0.01044857]\n",
      "auc 0.7368421052631579\n",
      "importances [0.4908353  0.49525423 0.01391047]\n",
      "auc 0.7368421052631579\n",
      "importances [0.49000081 0.49951832 0.01048087]\n",
      "auc 0.7368421052631579\n",
      "test group 2\n",
      "sizes of val sets 30 5\n",
      "sizes of train sets 122 20\n",
      "val group 0\n",
      "delta 4.0000000000000003e-07\n",
      "val group 1\n",
      "delta 4.0000000000000003e-07\n",
      "val group 2\n",
      "delta 4.0000000000000003e-07\n",
      "val group 3\n",
      "delta 4.0000000000000003e-07\n",
      "val group 4\n",
      "delta 4.0000000000000003e-07\n",
      "medians [0.53333333 0.58333333 0.58333333]\n",
      "test features: ['x0' 'x1']\n",
      "importances [0.49707985 0.50292015]\n",
      "auc 0.530701754385965\n",
      "importances [0.48284005 0.51715995]\n",
      "auc 0.6140350877192982\n",
      "importances [0.49221762 0.50778238]\n",
      "auc 0.530701754385965\n",
      "importances [0.49909752 0.50090248]\n",
      "auc 0.6140350877192982\n",
      "importances [0.50117325 0.49882675]\n",
      "auc 0.6140350877192982\n",
      "test group 3\n",
      "sizes of val sets 30 5\n",
      "sizes of train sets 122 20\n",
      "val group 0\n",
      "delta 4.0000000000000003e-07\n",
      "val group 1\n",
      "delta 4.0000000000000003e-07\n",
      "val group 2\n",
      "delta 4.0000000000000003e-07\n",
      "val group 3\n",
      "delta 4.0000000000000003e-07\n",
      "val group 4\n",
      "delta 4.0000000000000003e-07\n",
      "medians [0.5 0.5 0.5]\n",
      "test features: ['x0']\n",
      "importances [1.]\n",
      "auc 0.530701754385965\n",
      "importances [1.]\n",
      "auc 0.530701754385965\n",
      "importances [1.]\n",
      "auc 0.530701754385965\n",
      "importances [1.]\n",
      "auc 0.530701754385965\n",
      "importances [1.]\n",
      "auc 0.530701754385965\n",
      "test group 4\n",
      "sizes of val sets 30 5\n",
      "sizes of train sets 122 20\n",
      "val group 0\n",
      "delta 4.0000000000000003e-07\n",
      "val group 1\n",
      "delta 4.0000000000000003e-07\n",
      "val group 2\n",
      "delta 4.0000000000000003e-07\n",
      "val group 3\n",
      "delta 4.0000000000000003e-07\n",
      "val group 4\n",
      "delta 4.0000000000000003e-07\n",
      "medians [0.45 0.5  0.5 ]\n",
      "test features: ['x0' 'x1']\n",
      "importances [0.48821573 0.51178427]\n",
      "auc 0.6535087719298246\n",
      "importances [0.47918742 0.52081258]\n",
      "auc 0.5701754385964912\n",
      "importances [0.47903098 0.52096902]\n",
      "auc 0.5701754385964912\n",
      "importances [0.47608074 0.52391926]\n",
      "auc 0.4868421052631579\n",
      "importances [0.48856861 0.51143139]\n",
      "auc 0.6403508771929824\n",
      "test group 0\n",
      "sizes of val sets 30 5\n",
      "sizes of train sets 122 20\n",
      "val group 0\n",
      "delta 5e-07\n",
      "val group 1\n",
      "delta 5e-07\n",
      "val group 2\n",
      "delta 5e-07\n",
      "val group 3\n",
      "delta 5e-07\n",
      "val group 4\n",
      "delta 5e-07\n",
      "medians [0.45       0.58333333 0.58333333]\n",
      "test features: ['x0' 'x1']\n",
      "importances [0.49928245 0.50071755]\n",
      "auc 0.4605263157894737\n",
      "importances [0.49088107 0.50911893]\n",
      "auc 0.4736842105263158\n",
      "importances [0.4930051 0.5069949]\n",
      "auc 0.4473684210526316\n",
      "importances [0.49791781 0.50208219]\n",
      "auc 0.4605263157894737\n",
      "importances [0.49725636 0.50274364]\n",
      "auc 0.4605263157894737\n",
      "test group 1\n",
      "sizes of val sets 30 5\n",
      "sizes of train sets 122 20\n",
      "val group 0\n",
      "delta 4.0000000000000003e-07\n",
      "val group 1\n",
      "delta 4.0000000000000003e-07\n",
      "val group 2\n",
      "delta 4.0000000000000003e-07\n",
      "val group 3\n",
      "delta 4.0000000000000003e-07\n",
      "val group 4\n",
      "delta 4.0000000000000003e-07\n",
      "medians [0.48333333 0.48333333 0.58333333]\n",
      "test features: ['x0' 'x1' 'x2']\n",
      "importances [0.49106852 0.4964613  0.01247018]\n",
      "auc 0.7368421052631579\n",
      "importances [0.47607428 0.50800332 0.01592239]\n",
      "auc 0.7368421052631579\n",
      "importances [0.50678097 0.48277046 0.01044857]\n",
      "auc 0.7368421052631579\n",
      "importances [0.4908353  0.49525423 0.01391047]\n",
      "auc 0.7368421052631579\n",
      "importances [0.49000081 0.49951832 0.01048087]\n",
      "auc 0.7368421052631579\n",
      "test group 2\n",
      "sizes of val sets 30 5\n",
      "sizes of train sets 122 20\n",
      "val group 0\n",
      "delta 4.0000000000000003e-07\n",
      "val group 1\n",
      "delta 4.0000000000000003e-07\n",
      "val group 2\n",
      "delta 4.0000000000000003e-07\n",
      "val group 3\n",
      "delta 4.0000000000000003e-07\n",
      "val group 4\n",
      "delta 4.0000000000000003e-07\n",
      "medians [0.53333333 0.58333333 0.58333333]\n",
      "test features: ['x0' 'x1']\n",
      "importances [0.49707985 0.50292015]\n",
      "auc 0.530701754385965\n",
      "importances [0.48284005 0.51715995]\n",
      "auc 0.6140350877192982\n",
      "importances [0.49221762 0.50778238]\n",
      "auc 0.530701754385965\n",
      "importances [0.49909752 0.50090248]\n",
      "auc 0.6140350877192982\n",
      "importances [0.50117325 0.49882675]\n",
      "auc 0.6140350877192982\n",
      "test group 3\n",
      "sizes of val sets 30 5\n",
      "sizes of train sets 122 20\n",
      "val group 0\n",
      "delta 4.0000000000000003e-07\n",
      "val group 1\n",
      "delta 4.0000000000000003e-07\n",
      "val group 2\n",
      "delta 4.0000000000000003e-07\n",
      "val group 3\n",
      "delta 4.0000000000000003e-07\n",
      "val group 4\n",
      "delta 4.0000000000000003e-07\n",
      "medians [0.5 0.5 0.5]\n",
      "test features: ['x0']\n",
      "importances [1.]\n",
      "auc 0.530701754385965\n",
      "importances [1.]\n",
      "auc 0.530701754385965\n",
      "importances [1.]\n",
      "auc 0.530701754385965\n",
      "importances [1.]\n",
      "auc 0.530701754385965\n",
      "importances [1.]\n",
      "auc 0.530701754385965\n",
      "test group 4\n",
      "sizes of val sets 30 5\n",
      "sizes of train sets 122 20\n",
      "val group 0\n",
      "delta 4.0000000000000003e-07\n",
      "val group 1\n",
      "delta 4.0000000000000003e-07\n",
      "val group 2\n",
      "delta 4.0000000000000003e-07\n",
      "val group 3\n",
      "delta 4.0000000000000003e-07\n",
      "val group 4\n",
      "delta 4.0000000000000003e-07\n",
      "medians [0.45 0.5  0.5 ]\n",
      "test features: ['x0' 'x1']\n",
      "importances [0.48821573 0.51178427]\n",
      "auc 0.6535087719298246\n",
      "importances [0.47918742 0.52081258]\n",
      "auc 0.5701754385964912\n",
      "importances [0.47903098 0.52096902]\n",
      "auc 0.5701754385964912\n",
      "importances [0.47608074 0.52391926]\n",
      "auc 0.4868421052631579\n",
      "importances [0.48856861 0.51143139]\n",
      "auc 0.6403508771929824\n",
      "test group 0\n",
      "sizes of val sets 30 5\n",
      "sizes of train sets 122 20\n",
      "val group 0\n",
      "delta 5e-07\n",
      "val group 1\n",
      "delta 5e-07\n",
      "val group 2\n",
      "delta 5e-07\n",
      "val group 3\n",
      "delta 5e-07\n",
      "val group 4\n",
      "delta 5e-07\n",
      "medians [0.45       0.58333333 0.58333333]\n",
      "test features: ['x0' 'x1']\n",
      "importances [0.49928245 0.50071755]\n",
      "auc 0.4605263157894737\n",
      "importances [0.49088107 0.50911893]\n",
      "auc 0.4736842105263158\n",
      "importances [0.4930051 0.5069949]\n",
      "auc 0.4473684210526316\n",
      "importances [0.49791781 0.50208219]\n",
      "auc 0.4605263157894737\n",
      "importances [0.49725636 0.50274364]\n",
      "auc 0.4605263157894737\n",
      "test group 1\n",
      "sizes of val sets 30 5\n",
      "sizes of train sets 122 20\n",
      "val group 0\n",
      "delta 4.0000000000000003e-07\n",
      "val group 1\n",
      "delta 4.0000000000000003e-07\n",
      "val group 2\n",
      "delta 4.0000000000000003e-07\n",
      "val group 3\n",
      "delta 4.0000000000000003e-07\n",
      "val group 4\n",
      "delta 4.0000000000000003e-07\n",
      "medians [0.48333333 0.48333333 0.58333333]\n",
      "test features: ['x0' 'x1' 'x2']\n",
      "importances [0.49106852 0.4964613  0.01247018]\n",
      "auc 0.7368421052631579\n",
      "importances [0.47607428 0.50800332 0.01592239]\n",
      "auc 0.7368421052631579\n",
      "importances [0.50678097 0.48277046 0.01044857]\n",
      "auc 0.7368421052631579\n",
      "importances [0.4908353  0.49525423 0.01391047]\n",
      "auc 0.7368421052631579\n",
      "importances [0.49000081 0.49951832 0.01048087]\n",
      "auc 0.7368421052631579\n",
      "test group 2\n",
      "sizes of val sets 30 5\n",
      "sizes of train sets 122 20\n",
      "val group 0\n",
      "delta 4.0000000000000003e-07\n",
      "val group 1\n",
      "delta 4.0000000000000003e-07\n",
      "val group 2\n",
      "delta 4.0000000000000003e-07\n",
      "val group 3\n",
      "delta 4.0000000000000003e-07\n",
      "val group 4\n",
      "delta 4.0000000000000003e-07\n",
      "medians [0.53333333 0.58333333 0.58333333]\n",
      "test features: ['x0' 'x1']\n",
      "importances [0.49707985 0.50292015]\n",
      "auc 0.530701754385965\n",
      "importances [0.48284005 0.51715995]\n",
      "auc 0.6140350877192982\n",
      "importances [0.49221762 0.50778238]\n",
      "auc 0.530701754385965\n",
      "importances [0.49909752 0.50090248]\n",
      "auc 0.6140350877192982\n",
      "importances [0.50117325 0.49882675]\n",
      "auc 0.6140350877192982\n",
      "test group 3\n",
      "sizes of val sets 30 5\n",
      "sizes of train sets 122 20\n",
      "val group 0\n",
      "delta 4.0000000000000003e-07\n",
      "val group 1\n",
      "delta 4.0000000000000003e-07\n",
      "val group 2\n",
      "delta 4.0000000000000003e-07\n",
      "val group 3\n",
      "delta 4.0000000000000003e-07\n",
      "val group 4\n",
      "delta 4.0000000000000003e-07\n",
      "medians [0.5 0.5 0.5]\n",
      "test features: ['x0']\n",
      "importances [1.]\n",
      "auc 0.530701754385965\n",
      "importances [1.]\n",
      "auc 0.530701754385965\n",
      "importances [1.]\n",
      "auc 0.530701754385965\n",
      "importances [1.]\n",
      "auc 0.530701754385965\n",
      "importances [1.]\n",
      "auc 0.530701754385965\n",
      "test group 4\n",
      "sizes of val sets 30 5\n",
      "sizes of train sets 122 20\n",
      "val group 0\n",
      "delta 4.0000000000000003e-07\n",
      "val group 1\n",
      "delta 4.0000000000000003e-07\n",
      "val group 2\n",
      "delta 4.0000000000000003e-07\n",
      "val group 3\n",
      "delta 4.0000000000000003e-07\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 137\u001b[0m\n\u001b[1;32m    135\u001b[0m             rf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(random_state \u001b[38;5;241m=\u001b[39m seed, class_weight\u001b[38;5;241m=\u001b[39mclass_weight)\u001b[38;5;241m.\u001b[39mfit(stats_train[:,Index], label_train)\n\u001b[1;32m    136\u001b[0m             y_pred \u001b[38;5;241m=\u001b[39m rf\u001b[38;5;241m.\u001b[39mpredict(stats_val[:,Index])\n\u001b[0;32m--> 137\u001b[0m             aucs_val[VS, seed, j] \u001b[38;5;241m=\u001b[39m \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_median \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m: \u001b[38;5;66;03m#optimal number of features by median of mean\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedians\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mmedian(aucs_val, axis\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))) \u001b[38;5;66;03m#average by validation sets and random seeds\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/imputation/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/imputation/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:640\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    638\u001b[0m     labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_true)\n\u001b[1;32m    639\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m label_binarize(y_true, classes\u001b[38;5;241m=\u001b[39mlabels)[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 640\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_average_binary_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_binary_roc_auc_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_fpr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_fpr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# multilabel-indicator\u001b[39;00m\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _average_binary_score(\n\u001b[1;32m    649\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[38;5;241m=\u001b[39mmax_fpr),\n\u001b[1;32m    650\u001b[0m         y_true,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    653\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m    654\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/imputation/lib/python3.9/site-packages/sklearn/metrics/_base.py:76\u001b[0m, in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m format is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[1;32m     79\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/imputation/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:387\u001b[0m, in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y_true)) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    383\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly one class present in y_true. ROC AUC score \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    384\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis not defined in that case.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    385\u001b[0m     )\n\u001b[0;32m--> 387\u001b[0m fpr, tpr, _ \u001b[38;5;241m=\u001b[39m \u001b[43mroc_curve\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_fpr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m max_fpr \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m auc(fpr, tpr)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/imputation/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/imputation/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:1145\u001b[0m, in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m   1044\u001b[0m     {\n\u001b[1;32m   1045\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1054\u001b[0m     y_true, y_score, \u001b[38;5;241m*\u001b[39m, pos_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, drop_intermediate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m ):\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute Receiver operating characteristic (ROC).\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \n\u001b[1;32m   1058\u001b[0m \u001b[38;5;124;03m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;124;03m    array([ inf, 0.8 , 0.4 , 0.35, 0.1 ])\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1145\u001b[0m     fps, tps, thresholds \u001b[38;5;241m=\u001b[39m \u001b[43m_binary_clf_curve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;66;03m# Attempt to drop thresholds corresponding to points in between and\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;66;03m# collinear with other points. These are always suboptimal and do not\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;66;03m# appear on a plotted ROC curve (and thus do not affect the AUC).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;66;03m# but does not drop more complicated cases like fps = [1, 3, 7],\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m     \u001b[38;5;66;03m# tps = [1, 2, 4]; there is no harm in keeping too many thresholds.\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m drop_intermediate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fps) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/imputation/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:851\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    846\u001b[0m     weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;66;03m# y_score typically has many tied values. Here we extract\u001b[39;00m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;66;03m# the indices associated with the distinct values. We also\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;66;03m# concatenate a value for the end of the curve.\u001b[39;00m\n\u001b[0;32m--> 851\u001b[0m distinct_value_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiff\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_score\u001b[49m\u001b[43m)\u001b[49m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    852\u001b[0m threshold_idxs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mr_[distinct_value_indices, y_true\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    854\u001b[0m \u001b[38;5;66;03m# accumulate the true positives with decreasing threshold\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/imputation/lib/python3.9/site-packages/numpy/lib/_function_base_impl.py:1452\u001b[0m, in \u001b[0;36mdiff\u001b[0;34m(a, n, axis, prepend, append)\u001b[0m\n\u001b[1;32m   1450\u001b[0m op \u001b[38;5;241m=\u001b[39m not_equal \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mbool \u001b[38;5;28;01melse\u001b[39;00m subtract\n\u001b[1;32m   1451\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[0;32m-> 1452\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[43mslice1\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[43mslice2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "d_full = np.shape(species)[1]\n",
    "X_full, X_0, X_1, labels, labels_0, labels_1 = [[] for _ in range(6)] #X is data in 2 rows, labels are EPDS at tp 2\n",
    "ID = profile.Sample_id.to_numpy() #id of a sample in profile\n",
    "Individual_ID = Metadata.Individual_ID #id of a person\n",
    "for i in np.unique(Individual_ID):\n",
    "    TP = Metadata.TimePoint[Individual_ID == i].to_numpy()\n",
    "    if sum((TP - np.array([0,1,2]))**2)!=0:\n",
    "        warnings.warn(\"TP are missing or in incorrect order\") #does not happen in the dataset we have\n",
    "    Outcomes = EPDS[Individual_ID == i]\n",
    "    if Outcomes[2]!=2:\n",
    "        Reads = Metadata.ReadsNumber[Individual_ID == i].to_numpy()\n",
    "        Sample_ID = Metadata.Sample_ID[Individual_ID == i] #id of a sample in metadata\n",
    "        if 1-np.isnan(Reads[0]) and 1-np.isnan(Reads[1]): #complete data\n",
    "            labels = np.append(labels, Outcomes[2])\n",
    "            Dataset_line = np.zeros((1,2,d_full))\n",
    "            for j in range(2):\n",
    "                tp_id = Sample_ID[TP == j].to_numpy()\n",
    "                Dataset_line[:, j, :] = species[np.where(ID == tp_id)[0], :]\n",
    "            X_full = Dataset_line if np.size(X_full) == 0 else np.concatenate([X_full, Dataset_line])\n",
    "        if 1-np.isnan(Reads[0]) and np.isnan(Reads[1]): #tp 1 is missing\n",
    "            labels_0 = np.append(labels_0, Outcomes[2])\n",
    "            Dataset_line = np.zeros((1,2,d_full))\n",
    "            tp_id = Sample_ID[TP == 0].to_numpy()\n",
    "            Dataset_line[:, 0, :] = species[np.where(ID == tp_id)[0], :]\n",
    "            X_0 = Dataset_line if np.size(X_0) == 0 else np.concatenate([X_0, Dataset_line])\n",
    "        if np.isnan(Reads[0]) and 1-np.isnan(Reads[1]): #tp 0 is missing\n",
    "            labels_1 = np.append(labels_1, Outcomes[2])\n",
    "            Dataset_line = np.zeros((1,2,d_full))\n",
    "            tp_id = Sample_ID[TP == 1].to_numpy()\n",
    "            Dataset_line[:, 1, :] = species[np.where(ID == tp_id)[0], :]\n",
    "            X_1 = Dataset_line if np.size(X_1) == 0 else np.concatenate([X_1, Dataset_line])\n",
    "X_f, X_t = X_full[labels == 0], X_full[labels == 1] #f = healthy, t = depressed\n",
    "size_f, size_t = np.shape(X_f)[0], np.shape(X_t)[0]\n",
    "print(\"sizes of complete f and t sets:\",size_f,size_t)\n",
    "test_f, test_t = int(size_f*test), int(size_t*test) #amounts to put to test set\n",
    "print(\"sizes of test sets\",test_f, test_t)\n",
    "AUC_TEST, N_OPT, SENSITIVITY, SPECIFICITY = [[] for _ in range(4)]\n",
    "for IS in range(Imputation_sets):\n",
    "    sample_seed = IS + ITER\n",
    "    aucs_val = np.zeros((val_sets, ITER, n_features))\n",
    "    for TS in range(test_sets): #test sets\n",
    "        print(\"test group\", TS)\n",
    "        test_f_range, test_t_range = range(TS * test_f, (TS + 1) * test_f), range(TS * test_t, (TS + 1) * test_t)\n",
    "        rest_f_range, rest_t_range = np.setdiff1d(range(size_f), test_f_range), np.setdiff1d(range(size_t), test_t_range)\n",
    "        X_f_test, X_t_test = X_f[test_f_range], X_t[test_t_range]\n",
    "        X_test = np.concatenate([X_f_test,X_t_test])\n",
    "        label_test = np.zeros(test_f + test_t)\n",
    "        label_test[test_f:] = 1\n",
    "        rest_f, rest_t = np.size(rest_f_range), np.size(rest_t_range)\n",
    "        val_f, val_t = int(rest_f * val), int(rest_t * val) #amounts to put to validation set\n",
    "        print(\"sizes of val sets\",val_f, val_t)\n",
    "        train_f, train_t = rest_f - val_f, rest_t - val_t #the rest of rest is for training set\n",
    "        print(\"sizes of train sets\", train_f, train_t)\n",
    "        for VS in range(val_sets): #val sets\n",
    "            print(\"val group\", VS)\n",
    "            val_f_range, val_t_range = rest_f_range[range(VS* val_f, (VS + 1) * val_f)], rest_t_range[range(VS* val_t, (VS + 1) * val_t)]\n",
    "            train_f_range, train_t_range = np.setdiff1d(rest_f_range, val_f_range), np.setdiff1d(rest_t_range, val_t_range)\n",
    "            X_f_val, X_t_val, X_f_train, X_t_train = X_f[val_f_range], X_t[val_t_range], X_f[train_f_range], X_t[train_t_range]\n",
    "            X_val, X_train = np.concatenate([X_f_val, X_t_val]), np.concatenate([X_f_train, X_t_train])\n",
    "            label_val, label_train = np.zeros(val_f + val_t), np.zeros(train_f + train_t)\n",
    "            label_val[val_f:], label_train[train_f:] = 1, 1\n",
    "            X_test_cut, X_0_cut, X_1_cut = X_test, X_0, X_1\n",
    "            X = [X_train, X_val, X_test_cut, X_0_cut, X_1_cut]\n",
    "            d = np.shape(X[0])[2]\n",
    "            delta = np.min(X[0][X[0] > 0])\n",
    "            print(\"delta\", delta)  # the smallest element\n",
    "            for xi in range(len(X)):\n",
    "                X[xi] = (X[xi] + delta) / (1 + delta * d) #remove 0s\n",
    "            X_train, X_val, X_test_cut, X_0_cut, X_1_cut = X\n",
    "            ###classes\n",
    "            classes_train, classes_val, classes_test = np.ones((np.shape(X_train)[0],1)),np.ones((np.shape(X_val)[0],1)),np.ones((np.shape(X_test_cut)[0],1))\n",
    "            classes_0 = np.zeros((np.shape(X_0_cut)[0], 1))\n",
    "            #classes_1 = np.zeros((np.shape(X_1_cut)[0], 1))\n",
    "            ###imputation\n",
    "            X_0_cut = Imputation_library.impute(X_train, X_val, X_0_cut, 0, 1, sample_seed, arg_bacteria)\n",
    "            # X_1_cut = Imputation_library.impute(X_train, X_val, X_1_cut, 1, 0, sample_seed, arg_bacteria)\n",
    "            ###\n",
    "            X_train, classes_train = np.concatenate([X_train, X_0_cut]), np.concatenate([classes_train, classes_0]) ### concatenate full set with imputed sets\n",
    "            label_train = np.concatenate([label_train, labels_0])\n",
    "            ### feature extraction\n",
    "            stats_train, stats_val = Transformation_library.transform_2(X_train,  arg_bacteria), Transformation_library.transform_2(X_val,  arg_bacteria) ### construct features from 2 bacteria\n",
    "            stats_train, stats_val  = np.concatenate([stats_train, classes_train] ,axis = -1), np.concatenate([stats_val, classes_val], axis = -1)\n",
    "            n_features = np.min([n_features, np.shape(stats_train)[1]])\n",
    "            for j in range(n_features): #choose the best j + 1 features using filter method\n",
    "                sel = SelectKBest(f_classif, k=j + 1).fit(pd.DataFrame(stats_train), label_train)\n",
    "                Index = sel.get_support()\n",
    "                for seed in range(ITER): #classify by random forest with different random seeds\n",
    "                    rf = RandomForestClassifier(random_state = seed, class_weight=class_weight).fit(stats_train[:,Index], label_train)\n",
    "                    y_pred = rf.predict(stats_val[:,Index])\n",
    "                    aucs_val[VS, seed, j] = roc_auc_score(label_val, y_pred)\n",
    "        if is_median == 1: #optimal number of features by median of mean\n",
    "            print(\"medians\", np.median(aucs_val, axis=(0, 1))) #average by validation sets and random seeds\n",
    "            n_opt = np.argmax(np.median(aucs_val, axis=(0, 1))) + 1\n",
    "        else:\n",
    "            print(\"means\", np.mean(aucs_val, axis = (0,1)))\n",
    "            n_opt=np.argmax(np.mean(aucs_val, axis = (0,1))) + 1\n",
    "        N_OPT =np.append(N_OPT, n_opt)\n",
    "        stats_rest, label_rest = np.concatenate([stats_train, stats_val]), np.concatenate([label_train,label_val])#concatenate everything except testing set\n",
    "        stats_test = Transformation_library.transform_2(X_test_cut,  arg_bacteria)\n",
    "        stats_test = np.concatenate([stats_test, classes_test], axis = -1)\n",
    "        sel = SelectKBest(f_classif, k=n_opt).fit(pd.DataFrame(stats_rest), label_rest) #select optimal number of features\n",
    "        Index = sel.get_support()\n",
    "        print(\"test features:\",sel.get_feature_names_out()) #from features namas we can reconstruct what bacteria we use\n",
    "        for seed in range(ITER):\n",
    "            rf = RandomForestClassifier(random_state = seed, class_weight=class_weight).fit(stats_rest[:, Index], label_rest)\n",
    "            print(\"importances\",rf.feature_importances_) #display importances of features\n",
    "            y_pred = rf.predict(stats_test[:, Index])\n",
    "            print(\"auc\", roc_auc_score(label_test, y_pred)) #display balanced accuracy\n",
    "            AUC_TEST = np.append(AUC_TEST, roc_auc_score(label_test, y_pred)) # balanced accuracy on testing set\n",
    "            SENSITIVITY = np.append(SENSITIVITY, sum(y_pred[label_test==1]==1)/sum(label_test==1))\n",
    "            SPECIFICITY = np.append(SPECIFICITY, sum(y_pred[label_test==0]==0)/sum(label_test==0))\n",
    "print(\"mean auc\",np.mean(AUC_TEST))\n",
    "result= stats.ttest_1samp(AUC_TEST, level, alternative=\"greater\")\n",
    "print(\"p value\",result.pvalue)\n",
    "print(\"true positive\",np.mean(SENSITIVITY))\n",
    "print(\"true negative\",np.mean(SPECIFICITY))\n",
    "print(\"n opt\",np.mean(N_OPT))\n",
    "print(N_OPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed8ef45-9c93-4d1f-b2a2-f83dc3abed42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
