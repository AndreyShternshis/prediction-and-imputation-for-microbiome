{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5db858c-7a09-474b-a267-32bf55d7a615",
   "metadata": {},
   "outputs": [],
   "source": [
    "###import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_selection import (f_classif,SelectKBest)\n",
    "import scipy.stats as stats\n",
    "from scipy.cluster import hierarchy\n",
    "import warnings\n",
    "import sys\n",
    "sys.path.insert(0, '/libraries')\n",
    "from libraries import Transformation_library\n",
    "import os\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f65ddce-906c-43d4-ba28-6f2c7c40f89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###user-defined parameters\n",
    "is_balanced = 0 #do we want to use weights during classification?\n",
    "is_median = 1 #do we want to use median or mean when select optimal n (number of features)\n",
    "Dataset_N = 1 #number of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45105d29-95eb-4e0a-8fe5-fd18145c4aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "###standard parameters\n",
    "level = 0.5 #auc when guess is random\n",
    "test = 0.2 #fraction for testing\n",
    "val = 0.2 #fraction for validation (from the rest part)\n",
    "ITER = 5 #number of iteration for random forest\n",
    "n_features = 6 #the maximum number of features we consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e8e3e42-ecd9-49b9-bd04-d9f94d874cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sets = int(1/test) #number of testing sets (5)\n",
    "val_sets = int(1/val) #number of validation sets (5)\n",
    "if Dataset_N==1:\n",
    "    binary = 11 #threshold for depression\n",
    "    sparsity_level = 0.5 #sparsity threshold to merge bacteria together\n",
    "if Dataset_N==2:\n",
    "    sparsity_level = 0.9 #sparsity threshold to merge bacteria together\n",
    "if is_balanced == 1:\n",
    "    class_weight = \"balanced\"\n",
    "else:\n",
    "    class_weight = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e9cdb98-7ab5-4d7e-810c-32a0abe9249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###metadata\n",
    "if Dataset_N==1:\n",
    "    Metadata=pd.read_csv(\"BASIC_metadata_full.csv\",sep=',',low_memory=False) #read file with depression levels and ids of participants\n",
    "    Metadata.loc[Metadata.TimePoint==\"Trimester2\",\"TimePoint\"] = 0 #timepoits are 0,1,2\n",
    "    Metadata.loc[Metadata.TimePoint==\"Trimester3\",\"TimePoint\"] = 1\n",
    "    Metadata.loc[Metadata.TimePoint==\"PostpartumWeek6\",\"TimePoint\"] = 2\n",
    "    i = Metadata[Metadata.ReadsNumber<500000].index #remove insufficient reads\n",
    "    Metadata = Metadata.replace(Metadata.loc[i,'ReadsNumber'],np.nan)\n",
    "    EPDS = 2*np.ones_like(Metadata.EPDS) #2 is for missingness\n",
    "    EPDS[Metadata.EPDS>binary] = 1 #binary labels for depression\n",
    "    EPDS[Metadata.EPDS<=binary] = 0\n",
    "if Dataset_N==2:\n",
    "    Metadata=pd.read_csv(\"GMAP_metadata_public.csv\",sep=\",\",low_memory=False)\n",
    "    Metadata = Metadata.rename(columns={\"sample\": \"tp\"})\n",
    "    Metadata.loc[Metadata.tp==\"3_twomonth\",\"tp\"] = 0\n",
    "    Metadata.loc[Metadata.tp==\"4_fourmonth\",\"tp\"] = 1\n",
    "    Metadata.loc[Metadata.tp==\"5_sixmonth\",\"tp\"] = 2\n",
    "    time = [\".3.twomonth\", \".4.fourmonth\", \".5.sixmonth\"]\n",
    "    i = Metadata[pd.Series.isna(Metadata.case_id)].index #remove NaN values\n",
    "    Metadata = Metadata.drop(i)\n",
    "    AP = np.zeros_like(Metadata.case_id)\n",
    "    AP[Metadata.case_id.to_numpy()==\"AP Case\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c218686-9fcb-4c67-8ed0-d526c70e9895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 1.]\n",
      " [1. 0. 0. 1. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 1. 0. 0.]]\n",
      "sizes of complete f and t sets: 545 89\n",
      "sizes of test sets 109 17\n",
      "test group 0\n",
      "sizes of val sets 87 14\n",
      "sizes of train sets 349 58\n",
      "val group 0\n",
      "val group 1\n",
      "val group 2\n",
      "val group 3\n",
      "val group 4\n",
      "medians [0.5        0.53119869 0.58990148 0.59688013 0.59688013 0.59688013]\n",
      "test features: ['x0' 'x1' 'x3' 'x4']\n",
      "importances [0.07445169 0.23199815 0.19373461 0.49981555]\n",
      "auc 0.6038855909336212\n",
      "importances [0.07825634 0.27233732 0.20014566 0.44926067]\n",
      "auc 0.6038855909336212\n",
      "importances [0.07758671 0.1962811  0.22814402 0.49798817]\n",
      "auc 0.6038855909336212\n",
      "importances [0.08100014 0.25264352 0.1751421  0.49121425]\n",
      "auc 0.6038855909336212\n",
      "importances [0.08568441 0.23683103 0.2078934  0.46959115]\n",
      "auc 0.6038855909336212\n",
      "test group 1\n",
      "sizes of val sets 87 14\n",
      "sizes of train sets 349 58\n",
      "val group 0\n",
      "val group 1\n",
      "val group 2\n",
      "val group 3\n",
      "val group 4\n",
      "medians [0.71551724 0.71551724 0.71551724 0.63711002 0.63711002 0.63711002]\n",
      "test features: ['x4']\n",
      "importances [1.]\n",
      "auc 0.5671883432271991\n",
      "importances [1.]\n",
      "auc 0.5671883432271991\n",
      "importances [1.]\n",
      "auc 0.5671883432271991\n",
      "importances [1.]\n",
      "auc 0.5671883432271991\n",
      "importances [1.]\n",
      "auc 0.5671883432271991\n",
      "test group 2\n",
      "sizes of val sets 87 14\n",
      "sizes of train sets 349 58\n",
      "val group 0\n",
      "val group 1\n",
      "val group 2\n",
      "val group 3\n",
      "val group 4\n",
      "medians [0.71551724 0.71551724 0.60262726 0.60837438 0.60837438 0.60837438]\n",
      "test features: ['x4']\n",
      "importances [1.]\n",
      "auc 0.590124123043713\n",
      "importances [1.]\n",
      "auc 0.590124123043713\n",
      "importances [1.]\n",
      "auc 0.590124123043713\n",
      "importances [1.]\n",
      "auc 0.590124123043713\n",
      "importances [1.]\n",
      "auc 0.590124123043713\n",
      "test group 3\n",
      "sizes of val sets 87 14\n",
      "sizes of train sets 349 58\n",
      "val group 0\n",
      "val group 1\n",
      "val group 2\n",
      "val group 3\n",
      "val group 4\n",
      "medians [0.5        0.5        0.61986864 0.61986864 0.61986864 0.62561576]\n",
      "test features: ['x0' 'x1' 'x2' 'x3' 'x4' 'x5']\n",
      "importances [0.07288782 0.17675372 0.04888124 0.26875551 0.39113905 0.04158265]\n",
      "auc 0.5450620615218564\n",
      "importances [0.09026144 0.17123376 0.04611641 0.24226622 0.40169384 0.04842833]\n",
      "auc 0.5947112790070157\n",
      "importances [0.07964285 0.1742386  0.0405807  0.22884037 0.43484327 0.04185419]\n",
      "auc 0.5947112790070157\n",
      "importances [0.07593893 0.1931159  0.04295399 0.26201112 0.38076925 0.04521079]\n",
      "auc 0.5947112790070157\n",
      "importances [0.08901532 0.15551762 0.05375468 0.2668383  0.39320328 0.0416708 ]\n",
      "auc 0.5450620615218564\n",
      "test group 4\n",
      "sizes of val sets 87 14\n",
      "sizes of train sets 349 58\n",
      "val group 0\n",
      "val group 1\n",
      "val group 2\n",
      "val group 3\n",
      "val group 4\n",
      "medians [0.5        0.5        0.51272578 0.51272578 0.51272578 0.51272578]\n",
      "test features: ['x1' 'x3' 'x4']\n",
      "importances [0.31187659 0.35205975 0.33606366]\n",
      "auc 0.6287101996762008\n",
      "importances [0.30675109 0.35856187 0.33468704]\n",
      "auc 0.6287101996762008\n",
      "importances [0.27871078 0.33099947 0.39028975]\n",
      "auc 0.6287101996762008\n",
      "importances [0.30978947 0.3155876  0.37462293]\n",
      "auc 0.6332973556395035\n",
      "importances [0.3114696  0.30429077 0.38423964]\n",
      "auc 0.6332973556395035\n"
     ]
    }
   ],
   "source": [
    "X_full, labels = [[] for _ in range(2)] #X is 6 values denoting EPDS at tp 1 and 2 if available, labels are EPDS at tp 2\n",
    "Individual_ID = Metadata.Individual_ID #id of a person\n",
    "for i in np.unique(Individual_ID):\n",
    "    TP = Metadata.TimePoint[Individual_ID == i].to_numpy()\n",
    "    if sum((TP - np.array([0,1,2]))**2)!=0:\n",
    "        warnings.warn(\"TP are missing or in incorrect order\") #does not happen in the dataset we have\n",
    "    Outcomes = EPDS[Individual_ID == i]\n",
    "    if Outcomes[2]!=2:\n",
    "        Reads = Metadata.ReadsNumber[Individual_ID == i].to_numpy()\n",
    "        labels = np.append(labels, Outcomes[2])\n",
    "        Dataset_line = np.zeros((1,6))\n",
    "        Dataset_line[0,int(Outcomes[0])] = 1\n",
    "        Dataset_line[0,int(Outcomes[1]+3)] = 1\n",
    "        X_full = Dataset_line if np.size(X_full) == 0 else np.concatenate([X_full, Dataset_line])\n",
    "print(X_full)\n",
    "X_f, X_t = X_full[labels == 0], X_full[labels == 1] #f = healthy, t = depressed\n",
    "size_f, size_t = np.shape(X_f)[0], np.shape(X_t)[0]\n",
    "print(\"sizes of complete f and t sets:\",size_f,size_t)\n",
    "test_f, test_t = int(size_f*test), int(size_t*test) #amounts to put to test set\n",
    "print(\"sizes of test sets\",test_f, test_t)\n",
    "AUC_TEST, N_OPT, SENSITIVITY, SPECIFICITY = [[] for _ in range(4)]\n",
    "aucs_val = np.zeros((val_sets, ITER, n_features))\n",
    "for TS in range(test_sets): #test sets\n",
    "    print(\"test group\", TS)\n",
    "    test_f_range, test_t_range = range(TS * test_f, (TS + 1) * test_f), range(TS * test_t, (TS + 1) * test_t)\n",
    "    rest_f_range, rest_t_range = np.setdiff1d(range(size_f), test_f_range), np.setdiff1d(range(size_t), test_t_range)\n",
    "    X_f_test, X_t_test = X_f[test_f_range], X_t[test_t_range]\n",
    "    X_test = np.concatenate([X_f_test,X_t_test])\n",
    "    label_test = np.zeros(test_f + test_t)\n",
    "    label_test[test_f:] = 1\n",
    "    rest_f, rest_t = np.size(rest_f_range), np.size(rest_t_range)\n",
    "    val_f, val_t = int(rest_f * val), int(rest_t * val) #amounts to put to validation set\n",
    "    print(\"sizes of val sets\",val_f, val_t)\n",
    "    train_f, train_t = rest_f - val_f, rest_t - val_t #the rest of rest is for training set\n",
    "    print(\"sizes of train sets\", train_f, train_t)\n",
    "    for VS in range(val_sets): #val sets\n",
    "        print(\"val group\", VS)\n",
    "        val_f_range, val_t_range = rest_f_range[range(VS* val_f, (VS + 1) * val_f)], rest_t_range[range(VS* val_t, (VS + 1) * val_t)]\n",
    "        train_f_range, train_t_range = np.setdiff1d(rest_f_range, val_f_range), np.setdiff1d(rest_t_range, val_t_range)\n",
    "        X_f_val, X_t_val, X_f_train, X_t_train = X_f[val_f_range], X_t[val_t_range], X_f[train_f_range], X_t[train_t_range]\n",
    "        X_val, X_train = np.concatenate([X_f_val, X_t_val]), np.concatenate([X_f_train, X_t_train])\n",
    "        label_val, label_train = np.zeros(val_f + val_t), np.zeros(train_f + train_t)\n",
    "        label_val[val_f:], label_train[train_f:] = 1, 1\n",
    "        stats_train, stats_val = X_train, X_val\n",
    "        n_features = np.min([n_features, np.shape(stats_train)[1]])\n",
    "        for j in range(n_features): #choose the best j + 1 features using filter method\n",
    "            sel = SelectKBest(f_classif, k=j + 1).fit(pd.DataFrame(stats_train), label_train)\n",
    "            Index = sel.get_support()\n",
    "            for seed in range(ITER): #classify by random forest with different random seeds\n",
    "                rf = RandomForestClassifier(random_state = seed, class_weight=class_weight).fit(stats_train[:,Index], label_train)\n",
    "                y_pred = rf.predict(stats_val[:,Index])\n",
    "                aucs_val[VS, seed, j] = roc_auc_score(label_val, y_pred)\n",
    "    if is_median == 1: #optimal number of features by median of mean\n",
    "        print(\"medians\", np.median(aucs_val, axis=(0, 1))) #average by validation sets and random seeds\n",
    "        n_opt = np.argmax(np.median(aucs_val, axis=(0, 1))) + 1\n",
    "    else:\n",
    "        print(\"means\", np.mean(aucs_val, axis = (0,1)))\n",
    "        n_opt=np.argmax(np.mean(aucs_val, axis = (0,1))) + 1\n",
    "    N_OPT =np.append(N_OPT, n_opt)\n",
    "    X_rest, label_rest = np.concatenate([X_train, X_val]), np.concatenate([label_train,label_val])#concatenate everything except testing set\n",
    "    stats_rest, stats_test = X_rest, X_test\n",
    "    sel = SelectKBest(f_classif, k=n_opt).fit(pd.DataFrame(stats_rest), label_rest) #select optimal number of features\n",
    "    Index = sel.get_support()\n",
    "    print(\"test features:\",sel.get_feature_names_out()) #from features namas we can reconstruct what bacteria we use\n",
    "    for seed in range(ITER):\n",
    "        rf = RandomForestClassifier(random_state = seed, class_weight=class_weight).fit(stats_rest[:, Index], label_rest)\n",
    "        print(\"importances\",rf.feature_importances_) #display importances of features\n",
    "        y_pred = rf.predict(stats_test[:, Index])\n",
    "        print(\"auc\", roc_auc_score(label_test, y_pred)) #display balanced accuracy\n",
    "        AUC_TEST = np.append(AUC_TEST, roc_auc_score(label_test, y_pred)) # balanced accuracy on testing set\n",
    "        SENSITIVITY = np.append(SENSITIVITY, sum(y_pred[label_test==1]==1)/sum(label_test==1))\n",
    "        SPECIFICITY = np.append(SPECIFICITY, sum(y_pred[label_test==0]==0)/sum(label_test==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d96d63ee-c6f9-46b8-97f1-1a968713fbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean auc 0.5933189422558015\n",
      "p value 6.841197352749577e-16\n",
      "true positive 0.23764705882352938\n",
      "true negative 0.9489908256880735\n",
      "n opt 3.0\n",
      "[4. 1. 1. 6. 3.]\n"
     ]
    }
   ],
   "source": [
    "print(\"mean auc\",np.mean(AUC_TEST))\n",
    "result= stats.ttest_1samp(AUC_TEST, level, alternative=\"greater\")\n",
    "print(\"p value\",result.pvalue)\n",
    "print(\"true positive\",np.mean(SENSITIVITY))\n",
    "print(\"true negative\",np.mean(SPECIFICITY))\n",
    "print(\"n opt\",np.mean(N_OPT))\n",
    "print(N_OPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed8ef45-9c93-4d1f-b2a2-f83dc3abed42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
